{% macro layer_tf1x_gan(layer_name, skip_layer, random_data_layer, n_epochs, loss_function, class_weights, generator_optimizer, discriminator_optimiser, learning_rate, decay_steps, decay_rate, momentum, beta1, beta2, distributed, batch_size) %}
class {{layer_name}}(GANLayer):
    def __init__(self):
        self._n_epochs = {{n_epochs}}
        self._batch_size = {{batch_size}}

        self._stopped = False
        self._paused = False
        self._status = 'created'
        
        self._generator_loss_training = 0.0
        self._generator_loss_validation = 0.0
        self._generator_loss_testing = 0.0   

        self._discriminator_loss_training = 0.0
        self._discriminator_loss_validation = 0.0
        self._discriminator_loss_testing = 0.0   

        self._variables = {}
        self._generator_variables = {}
        self._discriminator_variables = {}

        self._gen_layer_outputs = {}
        self._gen_layer_weights = {}
        self._gen_layer_biases = {}        
        self._gen_layer_gradients = {}

        self._dis_layer_outputs = {}
        self._dis_layer_weights = {}
        self._dis_layer_biases = {}        
        self._dis_layer_gradients = {}


        self._layer_outputs = {}
        self._layer_weights = {}
        self._layer_biases = {}        
        self._layer_gradients = {}

        self._training_iteration = 0
        self._validation_iteration = 0
        self._testing_iteration = 0

        self._trn_sz_tot = 0
        self._val_sz_tot = 0
        self._tst_sz_tot = 0        
        
        self._fake_means = []
        self._fake_stds = []
        self._real_means = []
        self._real_stds = []

    def run(self, graph: Graph):
        """Called as the main entry point for training. Responsible for training the model.

        Args:
            graph: A PerceptiLabs Graph object containing references to all layers objects included in the model produced by this training layer.
        """        
        self._status = 'initializing'    
        random_data_layer_id = '{{random_layer}}'
        switch_layer_id = '{{switch_layer}}'    

        assert len(graph.data_nodes) == 2

        for node in graph.data_nodes():
            if node.layer_id != random_data_layer_id:
                training_data_node = node
            else:
                random_data_node = node

        training_node = graph.nodes[-1]
        switch_node = graph.get_node_by_id(switch_layer_id)
        output_node = graph.get_input_nodes(training_node)

        generator_nodes = graph._get_nodes_inbetween(random_data_node, switch_node)
        generator_layer_ids = [node.layer_id for node in generator_nodes]
        discriminator_nodes = graph._get_nodes_inbetween(switch_node, training_node)  
        discriminator_layer_ids = [node.layer_id for node in discriminator_nodes]
        real_data_nodes = graph._get_nodes_inbetween(training_data_node, switch_node)
        
        fake_data_path = graph._get_nodes_inbetween(random_data_node, training_node)
        real_data_path = graph._get_nodes_inbetween(training_data_node, training_node)

        self._trn_sz_tot = training_data_node.layer.size_training
        self._val_sz_tot = training_data_node.layer.size_validation
        self._tst_sz_tot = training_data_node.layer.size_testing
        
        # Make training set
        dataset_trn = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                training_data_node.layer_instance.make_generator_training,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.generate_data,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))

        # Make validation set
        dataset_val = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                training_data_node.layer_instance.make_generator_training,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.generate_data,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))

        # Make testing set
        dataset_tst = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                training_data_node.layer_instance.make_generator_training,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.generate_data,
                output_shapes=training_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))


        real_data_trn = dataset_trn.batch(self._batch_size)
        real_data_val = dataset_val.batch(self._batch_size)
        real_data_tst = dataset_tst.batch(1)                

        # Make initializers
        iterator = tf.data.Iterator.from_structure(dataset_trn.output_types, dataset_trn.output_shapes)
        trn_init = iterator.make_initializer(real_data_trn)
        val_init = iterator.make_initializer(real_data_val)
        tst_init = iterator.make_initializer(real_data_tst)        
        real_tensor , random_tensor = iterator.get_next()
        
        # Build the TensorFlow graph # TODO: perhaps this part can be delegated to the graph?
        fake_output_tensors = {
            random_data_node.layer_id: random_tensor
        }

        real_output_tensors = {
            training_data_node.layer_id: real_tensor
        }
        
        switch_node.layer_instance._selected_layer = generator_nodes[-1].layer_id 
        for node in fake_data_path:
            args = []
            for input_node in graph.get_input_nodes(node):
                args.append(generator_layer_output_tensors[input_node.layer_id])
            y = node.layer_instance(*args)
            fake_output_tensors[node.layer_id] = y

        if real_data_nodes != []:
            switch_node.layer_instance._selected_layer = real_data_nodes[-1].layer_id 
        else:
            switch_node.layer_instance._selected_layer = training_data_node.layer_id 
        for node in real_data_path:
            args = []
            for input_node in graph.get_input_nodes(node):
                args.append(layer_output_tensors[input_node.layer_id])
            y = node.layer_instance(*args)
            real_output_tensors[node.layer_id] = y

        real_tensor = real_output_tensors[output_node.layer_id]
        fake_tensor = fake_output_tensors[output_node.layer_id]

        self._real_input = real_tensor
        self._generated_output = fake_output_tensors[generator_nodes[-1].layer_id]

        def loss_func(logits_in,labels_in):
            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))

        dis_real_loss=loss_func(real_tensor,tf.ones_like(real_tensor)*0.9) #Smoothing for generalization
        dis_fake_loss=loss_func(fake_tensor,tf.zeros_like(real_tensor))
        
        dis_loss=dis_real_loss+dis_fake_loss
        gen_loss= loss_func(fake_tensor,tf.ones_like(fake_tensor))

        global_step = None
        {% filter remove_lspaces(8) %}        
            {% if generator_optimizer == 'tf.compat.v1.train.AdamOptimizer' %}
                generator_optimizer = tf.train.AdamOptimizer(learning_rate={{learning_rate}}, beta1={{beta1}}, beta2={{beta2}})       
                               
            {% else %}
                raise NotImplementedError('Optimizer {{optimizer}} not supported yet')
            {% endif %}
            {% if discriminator_optimizer == 'tf.compat.v1.train.AdamOptimizer' %}
                discriminator_optimizer = tf.train.AdamOptimizer(learning_rate={{learning_rate}}, beta1={{beta1}}, beta2={{beta2}})       
                               
            {% else %}
                raise NotImplementedError('Optimizer {{optimizer}} not supported yet')
            {% endif %}
        {% endfilter %}

        gen_layer_weight_tensors = {}
        gen_layer_bias_tensors = {}        
        gen_layer_gradient_tensors = {}

        dis_layer_weight_tensors = {}
        dis_layer_bias_tensors = {}        
        dis_layer_gradient_tensors = {}
        for node in generator_nodes:
            gen_layer_weight_tensors[node.layer_id] = node.layer.weights
            gen_layer_bias_tensors[node.layer_id] = node.layer.biases            
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                gen_layer_gradient_tensors[node.layer_id] = gradients
                self._layer_gradients[node.layer_id] = {name: [] for name in node.layer.trainable_variables.keys()} # Initialize
        
        for node in discriminator_nodes:
            dis_layer_weight_tensors[node.layer_id] = node.layer.weights
            dis_layer_bias_tensors[node.layer_id] = node.layer.biases            
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                dis_layer_gradient_tensors[node.layer_id] = gradients
                self._layer_gradients[node.layer_id] = {name: [] for name in node.layer.trainable_variables.keys()} # Initialize       

        for node in graph.inner_nodes:
            if node not in discriminator_nodes and not in generator_nodes:
                self._layer_weights[node.layer_id] = node.layer.weights
                self._layer_biases[node.layer_id] = node.layer.biases            
            
                if len(node.layer.trainable_variables) > 0:
                    gradients = {}
                    for name, tensor in node.layer.trainable_variables.items():
                        grad_tensor = tf.gradients(loss_tensor, tensor)
                        if any(x is None for x in grad_tensor):
                            grad_tensor = tf.constant(0)
                        gradients[name] = grad_tensor
                    layer_gradient_tensors[node.layer_id] = gradients
                    self._layer_gradients[node.layer_id] = {name: [] for name in node.layer.trainable_variables.keys()}
                

        dis_vars=[var for var in tvars if var.name in discriminator_layer_id for discriminator_layer_id in discriminator_layer_ids ]
        dis_grads = tf.gradients(dis_loss, dis_vars)
        dis_update_weights = discriminator_optimizer.apply_gradients(zip(dis_grads, dis_vars), global_step=global_step) 

        g_vars=[var for var in tvars if var.name in generator_layer_id for generator_layer_id in generator_layer_ids]
        gen_grads = tf.gradients(gen_loss, gen_vars)
        gen_update_weights = generator_optimizer.apply_gradients(zip(gen_grads, gen_vars), global_step=global_step) 

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)
        self._saver = tf.train.Saver()
        sess.run(tf.global_variables_initializer())

        def sleep_while_paused():
            while self._paused:
                time.sleep(1.0)

        def train_step():
            _, self._generator_loss_training, self._fake_outputs, self._gen_layer_weights,
             self._gen_layer_biases, self._gen_layer_gradients \
                = sess.run([
                    gen_update_weights, gen_loss, 
                    fake_output_tensors, gen_layer_weight_tensors, gen_layer_bias_tensors, gen_layer_gradient_tensors
                ])
            _, self._discriminator_loss_training, self._real_outputs, self._dis_layer_weights,
             self._dis_layer_biases, self._dis_layer_gradients \
                = sess.run([
                    dis_update_weights, dis_loss, 
                    real_output_tensors, dis_layer_weight_tensors, dis_layer_bias_tensors, dis_layer_gradient_tensors
                ])
            
        def validation_step():
            self._generator_loss_validation,  \
                self._fake_outputs, self._gen_layer_weights, self._gen_layer_biases, \
                self._gen_layer_gradients \
                = sess.run([
                    gen_loss, 
                    fake_output_tensors, gen_layer_weight_tensors, gen_layer_bias_tensors, gen_layer_gradient_tensors
                ])
            self._discriminator_loss_validation,  \
                self._real_outputs, self._dis_layer_weights, self._dis_layer_biases, \
                self._dis_layer_gradients \
                = sess.run([
                    dis_loss, 
                    real_output_tensors, dis_layer_weight_tensors, dis_layer_bias_tensors, dis_layer_gradient_tensors
                ])

            
        def test_step():
            self._generator_loss_testing, \
                self._fake_outputs, self._gen_layer_weights, gen_layer_gradients \
                = sess.run([
                    fake_loss,
                    fake_output_tensors, gen_layer_weight_tensors, gen_layer_gradient_tensors
                ])
            self._discriminator_loss_testing, \
                self._real_outputs, self._dis_layer_weights, dis_layer_gradients \
                = sess.run([
                    real_loss,
                    real_output_tensors, dis_layer_weight_tensors, dis_layer_gradient_tensors
                ])
            
        self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}

        log.info("Entering training loop")
        
        # Training loop
        self._epoch = 0
        while self._epoch < self._n_epochs:
            self._training_iteration = 0
            self._status = 'training'
            sess.run(trn_init)            
            try:
                while not self._stopped:
                    sleep_while_paused()
                    train_step()
                    self.save_snapshot(graph)
                    self._training_iteration += 1
            except tf.errors.OutOfRangeError:
                pass


            self._validation_iteration = 0
            self._status = 'validation'
            sess.run(val_init)            
            try:
                while not self._stopped:
                    sleep_while_paused()
                    validation_step()
                    self.save_snapshot(graph)                    
                    self._validation_iteration += 1
            except tf.errors.OutOfRangeError:
                pass


            log.info(
                f"Finished epoch {self._epoch+1}/{self._n_epochs} - "
                f"generator loss training, validation: {self.generator_loss_training:.6f}, {self.generator_loss_validation:.6f} - "
                f"discriminator loss training, validation: {self.discriminator_loss_training:.6f}, {self.discriminator_loss_validation:.6f}"
            )


            print(
                f"Finished epoch {self._epoch+1}/{self._n_epochs} - "
                f"generator loss training, validation: {self.generator_loss_training:.6f}, {self.generator_loss_validation:.6f} - "
                f"discriminator loss training, validation: {self.discriminator_loss_training:.6f}, {self.discriminator_loss_validation:.6f}"
            )

            self._epoch += 1

        self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
            
        # Test loop
        self._testing_iteration = 0
        self._status = 'testing'
        sess.run(tst_init)                                
        try:
            while not self._stopped:
                sleep_while_paused()
                test_step()
                self.save_snapshot(graph)                                    
                self._testing_iteration += 1
        except tf.errors.OutOfRangeError:
            pass

        self._status = 'finished'
        self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
        self.save_snapshot(graph)        

    def on_pause(self):
        """Called when the pause button is clicked in the frontend. 
        It is up to the implementing layer to pause its execution. 

        CAUTION: This method will be called from a different thread than run - keep thread-safety in mind."""
        self._paused = True

    def on_resume(self):
        """Called when the resume button is clicked in the frontend. 
        It is up to the implementing layer to resume execution. 
        
        CAUTION: This method will be called from a different thread than run - keep thread-safety in mind."""
        self._paused = False

    def on_save(self):
        """Called when the resume button is clicked in the frontend. 
        It is up to the implementing layer to save the model to disk.
        
        CAUTION: This method will be called from a different thread than run - keep thread-safety in mind."""
        # TODO: Call ._saver, verify thread-safety
        pass

    def on_stop(self):
        """Called when the save model button is clicked in the frontend. 
        It is up to the implementing layer to save the model to disk.
        
        CAUTION: This method will be called from a different thread than run - keep thread-safety in mind."""
        self._stopped = True

    @property
    def is_paused(self):
        """Returns true when the training is paused."""        
        return self._paused

    @property
    def batch_size(self):
        """ Size of the current training batch """        
        return self._batch_size

    @property
    def status(self):
        """Called when the pause button is clicked in the frontend. It is up to the implementing layer to pause its execution."""        
        return self._status
    
    @property
    def epoch(self):
        """The current epoch"""        
        return self._epoch

    @property
    def variables(self):
        """Any variables belonging to this layer that should be rendered in the frontend.
        
        Returns:
            A dictionary with tensor names for keys and picklable for values.
        """
        return self._variables.copy()        

    @property
    def sample(self) -> np.ndarray:
        """Returns a single data sample"""        
        return np.empty(())

    @property
    def size_training(self) -> int:
        """Returns the size of the training dataset"""                                    
        return self._trn_sz_tot

    @property
    def size_validation(self) -> int:
        """Returns the size of the validation dataset"""                                            
        return self._val_sz_tot

    @property
    def size_testing(self) -> int:
        """Returns the size of the testing dataset"""
        return self._tst_sz_tot

    def make_generator_training(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of training data. In the case of a training layer, this typically yields the model output."""        
        # Simply call sess.run on the output & target tensors :)  #TODO: how to make generators generic? We have two datasets here, but not all datasets will be labeled. Distinguish between supervised/unsupervised data layers and instead REQUIRE pairs of data layers for supervised?
        yield from []
        
    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of validation data. In the case of a training layer, this typically yields the model output."""                
        yield from []
        
    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of testing data. In the case of a training layer, this typically yields the model output."""                        
        yield from []


    @property
    def generator_loss_training(self) -> float:
        """Returns the current loss of the training phase"""                
        return self._generator_loss_training        

    @property
    def generator_loss_validation(self) -> float:
        """Returns the current loss of the validation phase"""                        
        return self._generator_loss_validation        

    @property
    def generator_loss_testing(self) -> float:
        """Returns the current loss of the testing phase"""                
        return self._generator_loss_testing
    
    @property
    def discriminator_loss_validation(self) -> float:
        """Returns the current loss of the validation phase"""                        
        return self._discriminator_loss_validation        

    @property
    def discriminator_loss_testing(self) -> float:
        """Returns the current loss of the testing phase"""                
        return self._discriminator_loss_testing

    @property
    def discriminator_loss_testing(self) -> float:
        """Returns the current loss of the testing phase"""                
        return self._discriminator_loss_testing

    @property
    def layer_weights(self) -> Dict[str, Dict[str, Picklable]]:
        """The weight values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """        
        return self._layer_weights

    @property
    def layer_biases(self) -> Dict[str, Dict[str, Picklable]]:
        """The bias values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """       
        self._layer_biases.update(self._gen_layer_biases)
        self._layer_biases.update(self._dis_layer_biases)
        return self._layer_biases
    
    @property
    def layer_gradients(self) -> Dict[str, Dict[str, Picklable]]:
        """The gradients with respect to the loss of all trainable variables of each layer in the input Graph.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain gradient name and value pairs. The values must be picklable.
        """        
        self._layer_gradients.update(self._gen_layer_gradients)
        self._layer_gradients.update(self._dis_layer_gradients)
        return self._layer_gradients
    
    @property
    def layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:
        """The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.
        """
        self._layer_outputs.update(self._fake_outputs)
        self._layer_outputs.update(self._real_outputs)
        return self._layer_outputs

    @property
    def training_iteration(self) -> int:
        """The current training iteration"""
        return self._training_iteration

    @property
    def validation_iteration(self) -> int:
        """The current validation iteration"""        
        return self._validation_iteration

    @property
    def testing_iteration(self) -> int:
        """The current testing iteration"""                
        return self._testing_iteration

    @property
    def get_real_input(self):
        return self._real_input

    @property
    def get_gengerated_output(self):
        return self._generated_output
    
    @property
    def get_generator_distribution(self):
        self._fake_means.append()
        self._fake_stds.append()
        return

    @property
    def get_real_distribution(self):
        self._real_means.append()
        self._real_stds.append()
        return

    @property
    def progress(self) -> float:
        """A number indicating the overall progress of the training
        
        Returns:
            A floating point number between 0 and 1
        """        
        n_iterations_per_epoch = np.ceil(self.size_training / self.batch_size) + \
                                 np.ceil(self.size_validation / self.batch_size)
        n_iterations_total = self._n_epochs * n_iterations_per_epoch

        iteration = self.epoch * n_iterations_per_epoch + \
                    self.training_iteration + self.validation_iteration
        
        progress = min(iteration/(n_iterations_total - 1), 1.0) 
        return progress
{% endmacro %}
