{% from 'tf1x_utils.j2' import session %}

{% macro run_normal(layer_name, switch_layer, real_layer, batch_size, n_epochs, class_weights, generator_optimizer, discriminator_optimizer, learning_rate, decay_steps, decay_rate, momentum, beta1, beta2, distributed, export_directory, use_cpu) %}
        self._status = 'initializing'

        real_data_layer_id = '{{real_layer}}' 
        switch_layer_id =  '{{switch_layer}}' 
        

        for node in graph.data_nodes:
            if node.layer_id != real_data_layer_id and not node.is_training_node:
                random_data_node = node
            elif node.layer_id == real_data_layer_id:
                real_data_node = node

        training_node = graph.nodes[-1]
        switch_node = graph.get_node_by_id(switch_layer_id)
        
        output_node = [node for node in graph.get_input_nodes(training_node)][0]
        output_layer_id = output_node.layer_id
        
        self._switch_layer_id = switch_layer_id
        self._selected_layer_id = switch_node.layer._selected_layer_id

        generator_nodes = graph.get_nodes_inbetween(random_data_node, switch_node)
        discriminator_nodes = graph.get_nodes_inbetween(switch_node, output_node)
        real_nodes = graph.get_nodes_inbetween(real_data_node, switch_node)

        generator_layer_ids = [node.layer_id for node in generator_nodes]
        discriminator_layer_ids = [node.layer_id for node in discriminator_nodes]

        self._generator_layer_ids = generator_layer_ids
        {# random_data_path = graph._get_nodes_inbetween(random_data_node, training_node) #}
        {# real_data_path = graph._get_nodes_inbetween(real_data_node, training_node) #}

        self._trn_sz_tot = real_data_node.layer.size_training
        self._val_sz_tot = real_data_node.layer.size_validation
        self._tst_sz_tot = real_data_node.layer.size_testing

        # Make training set
        dataset_trn = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                real_data_node.layer_instance.make_generator_training,
                output_shapes=real_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.make_generator_training,
                output_shapes=random_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))

        # Make validation set
        dataset_val = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                real_data_node.layer_instance.make_generator_validation,
                output_shapes=real_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.make_generator_validation,
                output_shapes=random_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))

        # Make testing set
        dataset_tst = tf.data.Dataset.zip((
            tf.data.Dataset.from_generator(
                real_data_node.layer_instance.make_generator_testing,
                output_shapes=real_data_node.layer_instance.sample.shape,
                output_types=np.float32                
            ),
            tf.data.Dataset.from_generator(
                random_data_node.layer_instance.make_generator_testing,
                output_shapes=random_data_node.layer_instance.sample.shape,
                output_types=np.float32
            )
        ))

        dataset_trn = dataset_trn.batch(self._batch_size)
        dataset_val = dataset_val.batch(self._batch_size)
        dataset_tst = dataset_tst.batch(1)

        # Make initializers
        iterator = tf.data.Iterator.from_structure(dataset_trn.output_types, dataset_trn.output_shapes)
        trn_init = iterator.make_initializer(dataset_trn)
        val_init = iterator.make_initializer(dataset_val)
        tst_init = iterator.make_initializer(dataset_tst)        
        real_tensor , random_tensor = iterator.get_next()

        # Build the TensorFlow graph # TODO: perhaps this part can be delegated to the graph?

        def build_real_graph(real_tensor):
            if len(real_nodes) > 1:
                switch_node.layer._selected_layer_id = real_nodes[-2].layer_id
            else:
                switch_node.layer._selected_layer_id = real_data_node.layer_id
            layer_output_tensors = {
                real_data_node.layer_id: real_tensor
            }
            for node in real_nodes:
                if len(list(graph.get_input_nodes(node))) <= 1:
                    args = []
                    for input_node in graph.get_input_nodes(node):
                        args.append(layer_output_tensors[input_node.layer_id])
                    y = node.layer_instance(*args)
                elif len(list(graph.get_input_nodes(node))) > 1:
                    args = {}
                    for input_node in graph.get_input_nodes(node):
                        if input_node in real_nodes:
                            args[input_node.layer_id] = layer_output_tensors[input_node.layer_id]
                        elif input_node == real_data_node:
                            args[real_data_node.layer_id] = layer_output_tensors[real_data_node.layer_id]
                    y = node.layer_instance(args)
                layer_output_tensors[node.layer_id] = y
            return layer_output_tensors

        def build_generator_graph(random_tensor):
            switch_node.layer._selected_layer_id = generator_nodes[-2].layer_id
            layer_output_tensors = {
                random_data_node.layer_id: random_tensor
            }
            for node in generator_nodes:
                if len(list(graph.get_input_nodes(node))) <= 1:
                    args = []
                    for input_node in graph.get_input_nodes(node):
                        args.append(layer_output_tensors[input_node.layer_id])
                    y = node.layer_instance(*args)
                elif len(list(graph.get_input_nodes(node))) > 1:
                    args = {}
                    for input_node in graph.get_input_nodes(node):
                        if input_node in generator_nodes:
                            args[input_node.layer_id] = layer_output_tensors[input_node.layer_id]
                    y = node.layer_instance(args)
                layer_output_tensors[node.layer_id] = y
            return layer_output_tensors
        
        def build_discriminator_graph(input_tensor):
            layer_output_tensors = {
                switch_node.layer_id: input_tensor
            }
            for node in discriminator_nodes:
                args = []
                for input_node in graph.get_input_nodes(node):
                    args.append(layer_output_tensors[input_node.layer_id])
                    y = node.layer_instance(*args)
                layer_output_tensors[node.layer_id] = y
            return layer_output_tensors

        generator_layer_output_tensors = build_generator_graph(random_tensor)
        real_layer_output_tensors = build_real_graph(real_tensor)
        
        real_output_tensor = real_layer_output_tensors[switch_layer_id]
        generator_output_tensor = generator_layer_output_tensors[switch_layer_id]
        
        real_discriminator_layer_output_tensors = build_discriminator_graph(real_output_tensor)
        random_discriminator_layer_output_tensors = build_discriminator_graph(generator_output_tensor)
        
        real_discriminator_output_tensor = real_discriminator_layer_output_tensors[output_layer_id]
        random_discriminator_output_tensor = random_discriminator_layer_output_tensors[output_layer_id]
        
        # Create an exportable version of the TensorFlow graph
        
        self._real_tensor_export = tf.placeholder(shape=dataset_trn.output_shapes[0], dtype=dataset_trn.output_types[0])
        self._random_tensor_export = tf.placeholder(shape=dataset_trn.output_shapes[1], dtype=dataset_trn.output_types[1])
        self._generator_output_tensor_export = build_generator_graph(self._random_tensor_export)[switch_layer_id]
        self._real_output_tensor_export = build_real_graph(self._real_tensor_export)[switch_layer_id]
        self._discriminator_real_output_tensor_export = build_discriminator_graph(self._real_output_tensor_export)[output_layer_id]
        self._discriminator_random_output_tensor_export = build_discriminator_graph(self._generator_output_tensor_export)[output_layer_id]
        
        #loss function

        def loss_func(logits_in,labels_in):
            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))
        
        discriminator_real_loss=loss_func(real_discriminator_output_tensor,tf.ones_like(real_discriminator_output_tensor)*0.9) 
        discriminator_random_loss=loss_func(random_discriminator_output_tensor,tf.zeros_like(random_discriminator_output_tensor))
        
        discriminator_loss_tensor = discriminator_real_loss+discriminator_random_loss
        generator_loss_tensor = loss_func(random_discriminator_output_tensor,tf.ones_like(random_discriminator_output_tensor))

        global_step = None
        {% filter remove_lspaces(8) %}        
            {% if generator_optimizer == 'tf.compat.v1.train.GradientDescentOptimizer' %}
                generator_optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate={{learning_rate}})
            {% elif generator_optimizer == 'tf.compat.v1.train.MomentumOptimizer' %}
                global_step = tf.Variable(0)
                learning_rate_momentum = tf.train.exponential_decay(
                    learning_rate={{learning_rate}},
                    global_step=global_step,
                    decay_steps={{decay_steps}},
                    decay_rate={{decay_rate}},
                    staircase=True
                )
                generator_optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate_momentum, momentum={{momentum}})
            {% elif generator_optimizer == 'tf.compat.v1.train.AdamOptimizer' %}
                generator_optimizer = tf.train.AdamOptimizer(learning_rate={{learning_rate}}, beta1={{beta1}}, beta2={{beta2}})
            {% elif generator_optimizer == 'tf.compat.v1.train.AdagradOptimizer' %}
                generator_optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate={{learning_rate}})            
            {% elif generator_optimizer == 'tf.compat.v1.train.RmsPropOptimizer' %}
                generator_optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate={{learning_rate}})                        
            {% else %}
                raise NotImplementedError('Optimizer {{generator_optimizer}} not supported yet')
            {% endif %}
        {% endfilter %}

        {% filter remove_lspaces(8) %}        
            {% if discriminator_optimizer == 'tf.compat.v1.train.GradientDescentOptimizer' %}
                discriminator_optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate={{learning_rate}})
            {% elif discriminator_optimizer == 'tf.compat.v1.train.MomentumOptimizer' %}
                global_step = tf.Variable(0)
                learning_rate_momentum = tf.train.exponential_decay(
                    learning_rate={{learning_rate}},
                    global_step=global_step,
                    decay_steps={{decay_steps}},
                    decay_rate={{decay_rate}},
                    staircase=True
                )
                discriminator_optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate_momentum, momentum={{momentum}})
            {% elif discriminator_optimizer == 'tf.compat.v1.train.AdamOptimizer' %}
                discriminator_optimizer = tf.train.AdamOptimizer(learning_rate={{learning_rate}}, beta1={{beta1}}, beta2={{beta2}})
            {% elif discriminator_optimizer == 'tf.compat.v1.train.AdagradOptimizer' %}
                discriminator_optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate={{learning_rate}})            
            {% elif discriminator_optimizer == 'tf.compat.v1.train.RmsPropOptimizer' %}
                discriminator_optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate={{learning_rate}})                        
            {% else %}
                raise NotImplementedError('Optimizer {{discriminator_optimizer}} not supported yet')
            {% endif %}
        {% endfilter %}

        generator_layer_weight_tensors = {}
        generator_layer_bias_tensors = {}        
        generator_layer_gradient_tensors = {}

        for node in generator_nodes+real_nodes:
            if not isinstance(node.layer, Tf1xLayer): # In case of pure custom layers...
                continue
            
            generator_layer_weight_tensors[node.layer_id] = node.layer.weights
            generator_layer_bias_tensors[node.layer_id] = node.layer.biases
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(generator_loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                generator_layer_gradient_tensors[node.layer_id] = gradients

        discriminator_layer_weight_tensors = {}
        discriminator_layer_bias_tensors = {}        
        discriminator_layer_gradient_tensors = {}

        for node in discriminator_nodes:
            if not isinstance(node.layer, Tf1xLayer): # In case of pure custom layers...
                continue
            
            discriminator_layer_weight_tensors[node.layer_id] = node.layer.weights
            discriminator_layer_bias_tensors[node.layer_id] = node.layer.biases
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(discriminator_loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                discriminator_layer_gradient_tensors[node.layer_id] = gradients

        
        trainable_vars = tf.trainable_variables() # TODO: safer to get from nodes. Especially with split graph in mind.
        
        discriminator_vars=[]
        for var in trainable_vars:
            for discriminator_layer_id in discriminator_layer_ids:
                if discriminator_layer_id in var.name:
                    discriminator_vars.append(var)

        discriminator_update_weights = discriminator_optimizer.minimize(discriminator_loss_tensor, var_list = discriminator_vars, global_step=global_step)

        generator_vars=[]
        for var in trainable_vars:
            for generator_layer_id in generator_layer_ids:
                if generator_layer_id in var.name:
                    generator_vars.append(var)

        generator_update_weights = generator_optimizer.minimize(generator_loss_tensor, var_list = generator_vars, global_step=global_step) 
             

        sess = None
        {{session(sess, use_gpu=not use_cpu) | indent(width=8)}}      
        self._sess = sess
        
        trackable_variables = {}
        trackable_variables.update({x.name: x for x in tf.trainable_variables() if isinstance(x, Trackable)})
        trackable_variables.update({k: v for k, v in locals().items() if isinstance(v, Trackable) and not isinstance(v, tf.python.data.ops.iterator_ops.Iterator)}) # TODO: Iterators based on 'stateful functions' cannot be serialized.
        self._checkpoint = tf.train.Checkpoint(**trackable_variables)
        sess.run(tf.global_variables_initializer())
        
        {% filter remove_lspaces(8) %}
            {% if export_directory is not none %}        
                path = tf.train.latest_checkpoint('{{export_directory}}')
                status = self._checkpoint.restore(path)
                status.assert_consumed().run_restore_ops(session=self._sess)
            {% endif %}
        {% endfilter %}                    
        
        def train_step():
            if not self._headless:
                _, self._generator_loss_training, self._generator_layer_outputs,\
                    self._generator_layer_weights, self._generator_layer_biases, \
                    self._generator_layer_gradients, self._real_layer_outputs, \
                _, self._discriminator_loss_training, self._random_discriminator_layer_outputs, self._real_discriminator_layer_outputs,\
                    self._discriminator_layer_weights, self._discriminator_layer_biases, \
                    self._discriminator_layer_gradients \
                    = sess.run([
                        generator_update_weights, generator_loss_tensor, generator_layer_output_tensors, \
                        generator_layer_weight_tensors, generator_layer_bias_tensors, generator_layer_gradient_tensors, real_layer_output_tensors, \
                        discriminator_update_weights, discriminator_loss_tensor, random_discriminator_layer_output_tensors, \
                        real_discriminator_layer_output_tensors, discriminator_layer_weight_tensors, discriminator_layer_bias_tensors, discriminator_layer_gradient_tensors
                    ])
                
            else:
                _, self._generator_loss_training, \
                    _, self.__discriminator_loss_training \
                    = sess.run([
                        generator_update_weights, generator_loss_tensor, \
                            discriminator_update_weights, discriminator_loss_tensor
                    ])

        def validation_step():
            if not self._headless:
                self._generator_loss_validation, self._generator_layer_outputs,\
                    self._generator_layer_weights, self._generator_layer_biases, \
                    self._generator_layer_gradients, self._real_layer_outputs, \
                    self._discriminator_loss_validation, self._discriminator_layer_outputs,\
                    self._discriminator_layer_weights, self._discriminator_layer_biases, \
                    self._discriminator_layer_gradients \
                    = sess.run([
                        generator_loss_tensor, generator_layer_output_tensors, generator_layer_weight_tensors, \
                        generator_layer_bias_tensors, generator_layer_gradient_tensors, real_layer_output_tensors, \
                        discriminator_loss_tensor, random_discriminator_layer_output_tensors, discriminator_layer_weight_tensors, \
                        discriminator_layer_bias_tensors, discriminator_layer_gradient_tensors
                    ])

            else:
                self._generator_loss_validation, \
                    self.__discriminator_loss_validation \
                    = sess.run(
                        generator_loss_tensor, \
                        discriminator_loss_tensor
                    )

        def test_step():
            self._generator_loss_testing, self._generator_layer_outputs,\
                self._generator_layer_weights, self._generator_layer_gradients, self._real_layer_outputs, \
                self._discriminator_loss_testing, self._discriminator_layer_outputs,\
                self._discriminator_layer_weights, self._discriminator_layer_gradients \
                = sess.run([
                    generator_loss_tensor, generator_layer_output_tensors, \
                    generator_layer_weight_tensors, generator_layer_gradient_tensors, real_layer_output_tensors, \
                    discriminator_loss_tensor, random_discriminator_layer_output_tensors, \
                    discriminator_layer_weight_tensors, discriminator_layer_gradient_tensors
                ])

        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}

        log.info("Entering training loop")

        # Training loop
        self._epoch = 0
        while self._epoch < self._n_epochs and not self._stopped:
            t0 = time.perf_counter()
            self._training_iteration = 0
            self._validation_iteration = 0
            self._status = 'training'
            sess.run(trn_init)            
            try:
                while not self._stopped:
                    train_step()
                    yield YieldLevel.SNAPSHOT
                    self._training_iteration += 1
            except tf.errors.OutOfRangeError:
                pass

            self._status = 'validation'
            sess.run(val_init)            
            try:
                while not self._stopped:
                    validation_step()
                    yield YieldLevel.SNAPSHOT                    
                    self._validation_iteration += 1
            except tf.errors.OutOfRangeError:
                pass
            log.info(
                f"Finished epoch {self._epoch+1}/{self._n_epochs} - "
                f"generator loss training, validation: {self.generator_loss_training:.6f}, {self.generator_loss_validation:.6f} - "
                f"discriminator loss training, validation: {self.discriminator_loss_training:.6f}, {self.discriminator_loss_validation:.6f} - "
            )
            log.info(f"Epoch duration: {round(time.perf_counter() - t0, 3)} s")  
            if self._stop_condition == "TargetAccuracy" and self._accuracy_training * 100 >= self._target_acc:
                break          
            self._epoch += 1

        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}            
        yield YieldLevel.DEFAULT            
        
        # Test loop
        self._testing_iteration = 0
        self._status = 'testing'
        sess.run(tst_init)                                
        try:
            while not self._stopped:
                test_step()
                yield YieldLevel.SNAPSHOT
                self._testing_iteration += 1
        except tf.errors.OutOfRangeError:
            pass

        self._status = 'finished'
        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}
        yield YieldLevel.DEFAULT
{% endmacro %}

################################################### Main #####################################################
{% macro layer_tf1x_gan(layer_name, switch_layer, real_layer, batch_size, n_epochs, target_acc, stop_condition, class_weights, generator_optimizer, discriminator_optimizer, learning_rate, decay_steps, decay_rate, momentum, beta1, beta2, distributed, export_directory, use_cpu) %}
class {{layer_name}}(GANLayer):

    def __init__(self):
        self._n_epochs = {{n_epochs}}
        self._batch_size = {{batch_size}}

        self._target_acc = {{target_acc}}
        self._stop_condition = '{{stop_condition}}'

        self._stopped = False
        self._paused = False
        self._headless = False
        self._status = 'created'
        
        self._generator_loss_training = 0.0
        self._generator_loss_validation = 0.0
        self._generator_loss_testing = 0.0   

        self._discriminator_loss_training = 0.0
        self._discriminator_loss_validation = 0.0
        self._discriminator_loss_testing = 0.0   

        self._variables = {}
        self._generator_variables = {}
        self._discriminator_variables = {}

        self._real_layer_outputs = {}
        self._generator_layer_outputs = {}
        self._generator_layer_weights = {}
        self._generator_layer_biases = {}        
        self._generator_layer_gradients = {}

        self._random_discriminator_layer_outputs = {}
        self._real_discriminator_layer_outputs = {}
        self._discriminator_layer_outputs = {}
        self._discriminator_layer_weights = {}
        self._discriminator_layer_biases = {}        
        self._discriminator_layer_gradients = {}

        self._layer_outputs = {}
        self._layer_weights = {}
        self._layer_biases = {}        
        self._layer_gradients = {}

        self._training_iteration = 0
        self._validation_iteration = 0
        self._testing_iteration = 0

        self._trn_sz_tot = 0
        self._val_sz_tot = 0
        self._tst_sz_tot = 0        
        
        self._random_means = []
        self._random_stds = []
        self._real_means = []
        self._real_stds = []

        self._checkpoint = None
        
    def run(self, graph: Graph):
        """Called as the main entry point for training. Responsible for training the model.

        Args:
            graph: A PerceptiLabs Graph object containing references to all layers objects included in the model produced by this training layer.
        """   
        {% if not distributed -%}
            {{ run_normal(layer_name, switch_layer, real_layer, batch_size, n_epochs, class_weights, generator_optimizer, discriminator_optimizer, learning_rate, decay_steps, decay_rate, momentum, beta1, beta2, distributed, export_directory, use_cpu) }}
        {% endif %}                

    def on_export(self, path: str, mode: str) -> None:
        """Called when the export or save button is clicked in the frontend.
        It is up to the implementing layer to save the model to disk.
        
        Args:
            path: the directory where the exported model will be stored.
            mode: how to export the model. Made available to frontend via 'export_modes' property."""

        log.debug(f"Export called. Project path = {path}, mode = {mode}")
        pb_path = os.path.join(path, '1')
        
        # Export non-compressed model
        if mode in ['TFModel', 'TFModel+checkpoint']:
            tf.compat.v1.saved_model.simple_save(self._sess, pb_path, \
            inputs={'real_input': self._real_tensor_export, 'random_input': self._random_tensor_export}, \
            outputs={'real_output': self._discriminator_real_output_tensor_export, 'random_output': self._discriminator_random_output_tensor_export })

        # Export compressed model
        if mode in ['TFLite', 'TFLite+checkpoint']:
            converter = tf.lite.TFLiteConverter.from_session(self._sess, [self._real_tensor_export, self._random_tensor_export], [self._discriminator_real_output_tensor_export, self._discriminator_random_output_tensor_export])
            converter.post_training_quantize = True
            tflite_model = converter.convert()
            open(pb_path, "wb").write(tflite_model)

        # Export checkpoint
        if mode in ['TFModel+checkpoint', 'TFLite+checkpoint']:
            {% filter remove_lspaces(8) %}
                {% if distributed %}
                    self._saver.save(self._sess, os.path.join(path, 'model.ckpt'), global_step=0)
                {% else %}
                    self._checkpoint.save(file_prefix=os.path.join(path, 'model.ckpt'), session=self._sess)
                {% endif %}
            {% endfilter %}
                
    def on_stop(self) -> None:
        """Called when the save model button is clicked in the frontend. 
        It is up to the implementing layer to save the model to disk."""
        self._stopped = True

    def on_headless_activate(self) -> None:
        """"Called when the statistics shown in statistics window are not needed.
        Purose is to speed up the iteration speed significantly."""
        self._headless = True

        self._layer_outputs = {} 
        self._layer_weights = {}
        self._layer_biases = {}
        self._layer_gradients = {}

    def on_headless_deactivate(self) -> None:
        """"Called when the statistics shown in statistics window are needed.
        May slow down the iteration speed of the training."""
        import time
        log.info(f"Set to headless_off at time {time.time()}")
        self._headless = False

    @property
    def export_modes(self) -> List[str]:
        """Returns the possible modes of exporting a model."""        
        return [
            'TFModel',
            'TFLite'
            'TFModel+checkpoint',
            'TFLite+checkpoint',            
        ]
        
    @property
    def is_paused(self) -> None:
        """Returns true when the training is paused."""        
        return self._paused

    @property
    def batch_size(self):
        """ Size of the current training batch """        
        return self._batch_size

    @property
    def status(self):
        """Called when the pause button is clicked in the frontend. It is up to the implementing layer to pause its execution."""        
        return self._status
    
    @property
    def epoch(self):
        """The current epoch"""        
        return self._epoch

    @property
    def variables(self):
        """Any variables belonging to this layer that should be rendered in the frontend.
        
        Returns:
            A dictionary with tensor names for keys and picklable for values.
        """
        return self._variables.copy()        

    @property
    def sample(self) -> np.ndarray:
        """Returns a single data sample"""        
        return np.empty(())

    @property
    def columns(self) -> List[str]: 
        """Column names. Corresponds to each column in a sample """
        return []
    
    @property
    def size_training(self) -> int:
        """Returns the size of the training dataset"""                                    
        return self._trn_sz_tot

    @property
    def size_validation(self) -> int:
        """Returns the size of the validation dataset"""                                            
        return self._val_sz_tot

    @property
    def size_testing(self) -> int:
        """Returns the size of the testing dataset"""
        return self._tst_sz_tot

    def make_generator_training(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of training data. In the case of a training layer, this typically yields the model output."""        
        # Simply call sess.run on the output & target tensors :)  #TODO: how to make generators generic? We have two datasets here, but not all datasets will be labeled. Distinguish between supervised/unsupervised data layers and instead REQUIRE pairs of data layers for supervised?
        yield from []
        
    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of validation data. In the case of a training layer, this typically yields the model output."""                
        yield from []
        
    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of testing data. In the case of a training layer, this typically yields the model output."""                        
        yield from []


    @property
    def get_switch_layer_id(self):
        """ Returns the layer name of the Switch layer"""
        return self._switch_layer_id
    
    @property
    def generator_loss_training(self) -> float:
        """Returns the current loss of the generator training phase"""                
        return self._generator_loss_training        

    @property
    def generator_loss_validation(self) -> float:
        """Returns the current loss of the generator validation phase"""                        
        return self._generator_loss_validation        

    @property
    def generator_loss_testing(self) -> float:
        """Returns the current loss of the generator testing phase"""                
        return self._generator_loss_testing
    
    @property
    def discriminator_loss_validation(self) -> float:
        """Returns the current loss of the discriminator validation phase"""                        
        return self._discriminator_loss_validation        

    @property
    def discriminator_loss_training(self) -> float:
        """Returns the current loss of the discriminator testing phase"""                
        return self._discriminator_loss_training

    @property
    def discriminator_loss_testing(self) -> float:
        """Returns the current loss of the discriminator testing phase"""                
        return self._discriminator_loss_testing

    @property
    def layer_weights(self) -> Dict[str, Dict[str, Picklable]]:
        """The weight values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """      
        self._layer_weights.update(self._generator_layer_weights)
        self._layer_weights.update(self._discriminator_layer_weights)   
        return self._layer_weights

    @property
    def layer_biases(self) -> Dict[str, Dict[str, Picklable]]:
        """The bias values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """       
        self._layer_biases.update(self._generator_layer_biases)
        self._layer_biases.update(self._discriminator_layer_biases) 
        return self._layer_biases
    
    @property
    def layer_gradients(self) -> Dict[str, Dict[str, Picklable]]:
        """The gradients with respect to the loss of all trainable variables of each layer in the input Graph.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain gradient name and value pairs. The values must be picklable.
        """
        self._layer_gradients.update(self._generator_layer_gradients)
        self._layer_gradients.update(self._discriminator_layer_gradients)       
        return self._layer_gradients
    
    @property
    def layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:
        """The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.
        """
        if self._selected_layer_id in self._generator_layer_ids:
            self._layer_outputs.update(self._real_layer_outputs)
            self._layer_outputs.update(self._generator_layer_outputs)
            self._layer_outputs.update(self._random_discriminator_layer_outputs)
        else:
            self._layer_outputs.update(self._generator_layer_outputs)
            self._layer_outputs.update(self._real_layer_outputs)
            self._layer_outputs.update(self._real_discriminator_layer_outputs)
        return self._layer_outputs

    @property
    def generator_layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:
        """The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.
        """
        return self._generator_layer_outputs
    
    @property
    def real_layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:
        """The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.
        """
        return self._real_layer_outputs

    @property
    def training_iteration(self) -> int:
        """The current training iteration"""
        return self._training_iteration

    @property
    def validation_iteration(self) -> int:
        """The current validation iteration"""        
        return self._validation_iteration

    @property
    def testing_iteration(self) -> int:
        """The current testing iteration"""                
        return self._testing_iteration
    
    @property
    def progress(self) -> float:
        """A number indicating the overall progress of the training
        
        Returns:
            A floating point number between 0 and 1
        """        
        n_iterations_per_epoch = np.ceil(self.size_training / self.batch_size) + \
                                 np.ceil(self.size_validation / self.batch_size)
        n_iterations_total = self._n_epochs * n_iterations_per_epoch

        iteration = self.epoch * n_iterations_per_epoch + \
                    self.training_iteration + self.validation_iteration
        
        progress = min(iteration/(n_iterations_total - 1), 1.0)
        return progress
{% endmacro %}
