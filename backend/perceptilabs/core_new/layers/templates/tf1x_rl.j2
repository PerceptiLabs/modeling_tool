{% from 'tf1x_utils.j2' import session %}

{% macro run_normal(layer_spec, graph_spec) %}
        self._status = 'initializing'
        copied_graph = graph.clone()
        target_nodes = copied_graph.inner_nodes
        online_nodes = graph.inner_nodes


        trn_node = graph.active_training_node
        output_nodes = graph.get_input_nodes(trn_node)
        for node in output_nodes:
            output_layer_id = node.layer_id
        online_output_layer_id = output_layer_id
        target_output_layer_id = output_layer_id

        input_data_node = graph.data_nodes[0]
        generator = input_data_node.layer_instance.make_generator()
        generator.send(None)
        state_ = input_data_node.layer_instance.reset_environment(generator)

        self._actions = input_data_node.layer_instance.action_space
        self._n_actions = len(self._actions)
        # create state tensor
        state_tensor = tf.placeholder(tf.float32, shape=(None,) + state_['output'].shape[:-1] + (state_['output'].shape[-1]*self._history_length, ))

        # Make initializers
        with tf.variable_scope('{{layer_spec.sanitized_name}}/train', reuse=tf.AUTO_REUSE):
            is_training = tf.get_variable(name="is_train", dtype=tf.bool, initializer=False)
        
        # Build the TensorFlow graph
        def build_graph(state_tensor, graph):
            layer_output_tensors = {
                input_data_node.layer_id: {'output': state_tensor},
            }

            for dst_node in graph.inner_nodes:
                inputs = {
                    dst_var: layer_output_tensors[src_node.layer_id][src_var]
                    for src_node, src_var, dst_var in graph.get_input_connections(dst_node)
                }
                y = dst_node.layer_instance(
                    inputs,
                    is_training=is_training
                )
                layer_output_tensors[dst_node.layer_id] = y

            return layer_output_tensors

        online_layer_output_tensors = build_graph(state_tensor, graph)
        target_layer_output_tensors = build_graph(state_tensor, copied_graph)


        Q_online_tensor = None
        Q_target_tensor = None
        
        for src_node, src_var, dst_var in graph.get_input_connections(graph.active_training_node):
            if dst_var == 'action':
                Q_online_tensor = online_layer_output_tensors[online_output_layer_id][src_var]
                Q_target_tensor = target_layer_output_tensors[target_output_layer_id][src_var]
                src_var_name = src_var
                
        if Q_online_tensor is None or Q_target_tensor is None:
            raise RuntimeError("Failed finding input layer!")

        
        # Exploration/exploitation tradeoff
        def epsilon(episode):
            eps = 1/np.sqrt(self._initial_exploration + episode)
            eps = self._final_exploration**(episode/self._final_exploration_frame)
            eps = max(0.1, eps)
            return eps
        
        
        input_shape = state_tensor.get_shape().as_list()[1:]
        y_tensor = tf.placeholder(tf.float32, [None, 1])
        a_tensor = tf.placeholder(tf.uint8, [None, 1])
        a_one_hot = tf.one_hot(a_tensor, self._n_actions, dtype=tf.float32)
        a_one_hot = a_one_hot[:, -1, :]
        q_performed = tf.reduce_sum(tf.multiply(Q_online_tensor , a_one_hot), axis=1, keep_dims=True)
        loss_tensor = tf.reduce_mean(tf.square(y_tensor - q_performed))
         

        # Create an exportable version of the TensorFlow graph
        self._input_tensor_export = tf.placeholder(shape=state_tensor.get_shape(), dtype=tf.float32)
        self._output_tensor_export = build_graph(
            self._input_tensor_export, graph)[online_output_layer_id]

        global_step = None
        {% filter remove_lspaces(8) %}
            {% if layer_spec.optimizer == 'SGD' %}
                optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate={{layer_spec.learning_rate}})
            {% elif layer_spec.optimizer == 'ADAM' %}
                optimizer = tf.train.AdamOptimizer(learning_rate={{layer_spec.learning_rate}}, beta1={{layer_spec.beta1}}, beta2={{layer_spec.beta2}})
            {% elif layer_spec.optimizer == 'adagrad' %}
                optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate={{layer_spec.learning_rate}})            
            {% elif layer_spec.optimizer == 'RMSprop' %}
                optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate={{layer_spec.learning_rate}})                        
                raise NotImplementedError('Optimizer {{layer_spec.optimizer}} not supported yet')
            {% endif %}
        {% endfilter %}

        online_layer_weight_tensors = {}
        online_layer_bias_tensors = {}        
        online_layer_gradient_tensors = {}

        target_layer_weight_tensors = {}
        target_layer_bias_tensors = {}        
        target_layer_gradient_tensors = {}

        for node in graph.inner_nodes:
            if not isinstance(node.layer, Tf1xLayer): 
                continue
            online_layer_weight_tensors[node.layer_id] = node.layer.weights
            online_layer_bias_tensors[node.layer_id] = node.layer.biases            
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                online_layer_gradient_tensors[node.layer_id] = gradients
                
        for node in copied_graph.inner_nodes:
            if not isinstance(node.layer, Tf1xLayer): 
                continue
            target_layer_weight_tensors[node.layer_id] = node.layer.weights
            target_layer_bias_tensors[node.layer_id] = node.layer.biases            
            
            if len(node.layer.trainable_variables) > 0:
                gradients = {}
                for name, tensor in node.layer.trainable_variables.items():
                    grad_tensor = tf.gradients(loss_tensor, tensor)
                    if any(x is None for x in grad_tensor):
                        grad_tensor = tf.constant(0)
                    gradients[name] = grad_tensor
                target_layer_gradient_tensors[node.layer_id] = gradients

        
        grads_and_vars = optimizer.compute_gradients(loss_tensor)
        update_online_weights = optimizer.apply_gradients(grads_and_vars, global_step=global_step)
        
        def copy_weights(sess):
            update_ops = []
            online_variables = [var for var in tf.trainable_variables() if 'copy' not in var.name]
            online_variables = sorted(online_variables, key=lambda v: v.name)
            target_variables = [var for var in tf.trainable_variables() if 'copy' in var.name]
            target_variables = sorted(target_variables, key=lambda v: v.name)
            for v1, v2 in zip(target_variables, online_variables):
                op = v1.assign(v2)
                update_ops.append(op)
            sess.run(update_ops)


        sess = None
        {{session(sess, use_gpu=not layer_spec.use_cpu) | indent(width=8)}}        
        self._sess = sess

        trackable_variables = {}
        trackable_variables.update({x.name: x for x in tf.trainable_variables() if isinstance(x, Trackable)})
        trackable_variables.update({k: v for k, v in locals().items() if isinstance(v, Trackable) and not isinstance(v, tf.python.data.ops.iterator_ops.Iterator)}) # TODO: Iterators based on 'stateful functions' cannot be serialized.
        self._checkpoint = tf.train.Checkpoint(**trackable_variables)
        sess.run(tf.global_variables_initializer())
        
        {% filter remove_lspaces(8) %}
            {% if layer_spec.checkpoint_path is not none %}        
                path = tf.train.latest_checkpoint('{{layer_spec.checkpoint_path}}')
                status = self._checkpoint.restore(path)
                status.assert_consumed().run_restore_ops(session=self._sess)
            {% endif %}
        {% endfilter %}
        

        log.info("Entering training loop")

        # Training loop


        self._replay_memory = []
        self._episode = 0
        iterations = 0

        while self._episode < self._n_episodes and not self._stopped:
            t0 = time.perf_counter()
            self._status = 'training'
            state_ = input_data_node.layer_instance.reset_environment(generator)
            state_seq = [state_['output']]*self._history_length
            self._step_counter = 0
            done = False
            self._reward = 0
            try:
                while (not self._stopped) and (self._step_counter < self._n_steps_max) and not done:

                    explore = np.random.random() < epsilon(self._episode) or iterations < self._replay_start_size
                    if explore:
                        action = np.random.randint(0, self._n_actions)
                        probs = [1/self._n_actions for i in range(self._n_actions)]
                    else:
                        Q = Q_online_tensor.eval(session=sess, feed_dict={state_tensor: [np.concatenate(state_seq,-1)]}).squeeze()
                        action = np.argmax(Q)
                        probs = [i/np.sum(Q) for i in Q]
                    state_info = input_data_node.layer_instance.take_action(generator, action)
                    new_state, reward, done, info = state_info['output'], state_info['reward'], state_info['done'], state_info['info']
                    new_state_seq = state_seq[1:] + [new_state]
                    self._reward = reward

                    transition = {
                        'state_seq': np.array(state_seq),
                        'new_state_seq': np.array(new_state_seq),
                        'action': action,
                        'reward': reward,
                        'done': done,
                        'probs':probs
                        }
                    self._replay_memory.append(transition)
                                        
                    if len(self._replay_memory) > self._replay_memory_size:
                        self._replay_memory.pop(0)

                    state_seq = new_state_seq
                    
                    if iterations % 4 == 0 and iterations > self._replay_start_size:
                        
                        batch_transitions = np.random.choice(self._replay_memory, self._batch_size)
                        y_batch = np.zeros((self._batch_size, 1))
                        a_batch = np.zeros((self._batch_size, 1))
                        X_batch = np.zeros((self._batch_size,) + tuple(input_shape))

                        for i, t in enumerate(batch_transitions):
                            y_batch[i] = t['reward']
                            if not t['done']:
                                feed_dict = {state_tensor: [np.concatenate(t['new_state_seq'],-1)]}
                                Q = Q_target_tensor.eval(feed_dict=feed_dict, session=sess).squeeze()
                                y_batch[i] += self._gamma*np.amax(Q)
                            a_batch[i] = t['action']
                            X_batch[i] = np.concatenate(t['state_seq'],-1)
                        
                        feed_dict = {
                         state_tensor: np.atleast_2d(X_batch),
                         a_tensor: np.atleast_2d(a_batch),
                         y_tensor: np.atleast_2d(y_batch),
                        }
                        
                        # update this line
                        _, self._loss_training, self._online_layer_outputs,\
                        self._online_layer_weights, self._online_layer_biases, self._online_layer_gradients \
                        = sess.run([update_online_weights, loss_tensor,
                            online_layer_output_tensors, online_layer_weight_tensors, online_layer_bias_tensors, online_layer_gradient_tensors
                        ], feed_dict=feed_dict)
                    # Copy weights
                    if iterations % 100 == 0:
                        copy_weights(sess)
                    state_ = new_state

                    yield YieldLevel.SNAPSHOT
                    self._step_counter += 1

                    
                    iterations += 1

            except tf.errors.OutOfRangeError:
                pass
            
            log.info(f"Episode duration: {round(time.perf_counter() - t0, 3)} s")            
            self._episode += 1

        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}            
        yield YieldLevel.DEFAULT            

        self._status = 'finished'
        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}
        yield YieldLevel.DEFAULT
{% endmacro %}

################################################### Main #####################################################
{% macro layer_tf1x_rl(layer_spec, graph_spec) %}
class {{layer_spec.sanitized_name}}(RLLayer):

    def __init__(self):

        self._batch_size = {{layer_spec.batch_size}}
        self._n_episodes = {{layer_spec.n_episodes}}
        self._history_length = {{layer_spec.history_length}}
        self._gamma = {{layer_spec.discount_factor}}

        self._replay_start_size = 2*self._batch_size
        self._replay_memory_size = {{layer_spec.replay_memory_size}}

        self._initial_exploration = {{layer_spec.initial_exploration}}
        self._final_exploration = {{layer_spec.final_exploration}}
        self._final_exploration_frame = {{layer_spec.final_exploration_frame}}

        self._update_frequency = {{layer_spec.update_frequency}}
        self._copy_weights_frequency = {{layer_spec.target_network_update_frequency}}
        self._n_steps_max = {{layer_spec.n_steps_max}}

        self._stopped = False
        self._paused = False
        self._headless = False
        self._status = 'created'
        
        self._loss_training = 0.0
        self._loss_validation = 0.0
        self._loss_testing = 0.0 

        self._variables = {}
        self._layer_outputs = {}
        self._layer_weights = {}
        self._layer_biases = {}        
        self._layer_gradients = {}

        self._online_layer_outputs = {}
        self._online_layer_weights = {}
        self._online_layer_biases = {}        
        self._online_layer_gradients = {}

        self._target_layer_outputs = {}
        self._target_layer_weights = {}
        self._target_layer_biases = {}        
        self._target_layer_gradients = {}


        self._checkpoint = None
        
    def run(self, graph: Graph):
        """Called as the main entry point for training. Responsible for training the model.

        Args:
            graph: A PerceptiLabs Graph object containing references to all layers objects included in the model produced by this training layer.
        """   
        {% if not layer_spec.distributed -%}
            {{ run_normal(layer_spec, graph_spec) }}
        {% endif %}                

    def on_export(self, path: str, mode: str) -> None:
        """Called when the export or save button is clicked in the frontend.
        It is up to the implementing layer to save the model to disk.
        
        Args:
            path: the directory where the exported model will be stored.
            mode: how to export the model. Made available to frontend via 'export_modes' property."""

        log.debug(f"Export called. Project path = {path}, mode = {mode}")
        pb_path = os.path.join(path, '1')
        
        # Export non-compressed model
        if mode in ['TFModel', 'TFModel+checkpoint']:
            tf.compat.v1.saved_model.simple_save(self._sess, pb_path, inputs={'input': self._input_tensor_export}, outputs={'output': self._output_tensor_export})

        # Export compressed model
        if mode in ['TFLite', 'TFLite+checkpoint']:
            converter = tf.lite.TFLiteConverter.from_session(self._sess, [self._input_tensor_export], [self._output_tensor_export])
            converter.post_training_quantize = True
            tflite_model = converter.convert()
            open(pb_path, "wb").write(tflite_model)

        # Export checkpoint
        if mode in ['TFModel+checkpoint', 'TFLite+checkpoint']:
            {% filter remove_lspaces(8) %}
                {% if layer_spec.distributed %}
                    self._saver.save(self._sess, os.path.join(path, 'model.ckpt'), global_step=0)
                {% else %}
                    self._checkpoint.save(file_prefix=os.path.join(path, 'model.ckpt'), session=self._sess)
                {% endif %}
            {% endfilter %}
                
    def on_stop(self) -> None:
        """Called when the save model button is clicked in the frontend. 
        It is up to the implementing layer to save the model to disk."""
        self._stopped = True

    def on_headless_activate(self) -> None:
        """"Called when the statistics shown in statistics window are not needed.
        Purose is to speed up the iteration speed significantly."""
        self._headless = True

        self._layer_outputs = {} 
        self._layer_weights = {}
        self._layer_biases = {}
        self._layer_gradients = {}

    def on_headless_deactivate(self) -> None:
        """"Called when the statistics shown in statistics window are needed.
        May slow down the iteration speed of the training."""
        import time
        log.info(f"Set to headless_off at time {time.time()}")
        self._headless = False

    @property
    def export_modes(self) -> List[str]:
        """Returns the possible modes of exporting a model."""        
        return [
            'TFModel',
            'TFLite'
            'TFModel+checkpoint',
            'TFLite+checkpoint',            
        ]
        
    @property
    def is_paused(self) -> None:
        """Returns true when the training is paused."""        
        return self._paused

    @property
    def batch_size(self) -> int:
        """ Size of the current training batch """        
        return self._batch_size

    @property
    def status(self):
        """Called when the pause button is clicked in the frontend. It is up to the implementing layer to pause its execution."""        
        return self._status
    
    @property
    def episode(self) -> int:
        """The current epoch"""        
        return self._episode

    @property
    def n_episodes(self) -> int:
        """The current epoch"""        
        return self._n_episodes

    @property
    def gamma(self) -> float:
        """ gamma """
        return self._gamma

    @property
    def replay_memory_size(self) -> int:
        """ replay memory size """
        return self._replay_memory_size

    @property
    def transition(self) -> Dict[str, Picklable]:
        """ replay memory """
        return self._replay_memory[-1]

    @property
    def n_steps_max(self) -> int:
        """ _n_steps_max """
        return self._n_steps_max

    @property
    def step_counter(self) -> int:
        """ _step_counter """
        return self._step_counter

    @property
    def history_length(self) -> int:
        """ history length"""
        return self._history_length

    @property
    def variables(self):
        """Any variables belonging to this layer that should be rendered in the frontend.
        
        Returns:
            A dictionary with tensor names for keys and picklable for values.
        """
        return self._variables.copy()        

    @property
    def sample(self) -> np.ndarray:
        """Returns a single data sample"""        
        return {}

    @property
    def action_space(self) -> List[int]:
        """ Returns the action space of the environment used for training the model"""
        return self._actions

    @property
    def n_actions(self) -> float:
        """ returns reward during one iteration"""
        return self._n_actions

    @property
    def reward(self) -> float:
        """ returns reward during one iteration"""
        return self._reward

    @property
    def loss_training(self) -> float:
        """Returns the current loss of the training phase"""                
        return self._loss_training        

    @property
    def loss_validation(self) -> float:
        """Returns the current loss of the validation phase"""                        
        return self._loss_validation        

    @property
    def loss_testing(self) -> float:
        """Returns the current loss of the testing phase"""                
        return self._loss_testing


    def make_generator(self) -> Generator[np.ndarray, None, None]:
        """Returns a generator yielding single samples of training data. In the case of a training layer, this typically yields the model output."""        
        # Simply call sess.run on the output & target tensors :)  #TODO: how to make generators generic? We have two datasets here, but not all datasets will be labeled. Distinguish between supervised/unsupervised data layers and instead REQUIRE pairs of data layers for supervised?
        yield from []

    def reset_environment(self, generator) -> np.ndarray:
        """  Returns the state of the environment after being reset"""
        return generator.send('reset')

    def take_action(self, generator, action) -> List:
        """ Takes the given action in the environment and returns the new state info. """
        return generator.send(action)

    @property
    def layer_weights(self) -> Dict[str, Dict[str, Picklable]]:
        """The weight values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """        
        self._layer_weights.update(self._online_layer_weights)  
        return self._layer_weights

    @property
    def layer_biases(self) -> Dict[str, Dict[str, Picklable]]:
        """The bias values of each layer in the input Graph during the training.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.
        """      
        self._layer_biases.update(self._online_layer_biases)  
        return self._layer_biases
    
    @property
    def layer_gradients(self) -> Dict[str, Dict[str, Picklable]]:
        """The gradients with respect to the loss of all trainable variables of each layer in the input Graph.

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain gradient name and value pairs. The values must be picklable.
        """        
        self._layer_gradients.update(self._online_layer_gradients)
        return self._layer_gradients
    
    @property
    def layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:
        """The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)

        Returns:
            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.
        """
        self._layer_outputs.update(self._online_layer_outputs)
        return self._layer_outputs

    @property
    def progress(self) -> float:
        """A number indicating the overall progress of the training
        
        Returns:
            A floating point number between 0 and 1
        """        
        n_steps_episode = self._n_steps_max
        n_iterations_total = self._n_episodes * n_steps_episode

        iteration = self.episode * n_steps_episode + self.step_counter
        
        progress = min(iteration/(n_iterations_total - 1), 1.0) 
        return progress
        
{% endmacro %}
