{% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
class {{layer_name}}(Tf1xLayer):
    def __call__(self, x):
        y = tf.reshape(x, [-1] + {{shape}})
        y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
        return y

    @property
    def variables(self):
        return {}

    @property
    def trainable_variables(self):
        return {}

    @property
    def weights(self):
        return {}

    @property
    def biases(self):
        return {}        
{% endmacro %}


{% macro layer_tf1x_one_hot(layer_name, n_classes) %}
class {{layer_name}}(Tf1xLayer):
    def __call__(self, x):
        y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
        return y

    @property
    def variables(self):
        return {}

    @property
    def trainable_variables(self):
        return {}

    @property
    def weights(self):
        return {}

    @property
    def biases(self):
        return {}    
{% endmacro %}


{% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
class {{layer_name}}(Tf1xLayer):
    def __init__(self):
        self._scope = '{{layer_name}}'
        self._n_neurons = 10
        self._variables = {}
        
    def __call__(self, x):
        n_neurons = {{n_neurons}}
        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)

        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
            W = tf.compat.v1.get_variable('W', initializer=initial)
        
            initial = tf.constant(0.1, shape=[self._n_neurons])
            b = tf.compat.v1.get_variable('b', initializer=initial)
            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
            y = tf.matmul(flat_node, W) + b

            {% filter remove_lspaces(8) %}
                {% if activation is not none %}
                    y = {{activation}}(y)
                {% endif %}
            {% endfilter %}
            
        self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
        return y

    @property
    def variables(self):
        return self._variables.copy()

    @property
    def trainable_variables(self):
        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
        variables = {v.name: v for v in variables}
        return variables

    @property
    def weights(self):
        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
            w = tf.compat.v1.get_variable('W')
            return {w.name: w}

    @property
    def biases(self):
        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
            b = tf.compat.v1.get_variable('b')
            return {b.name: b}
    
{% endmacro %}


{% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_area, pool_stride) %}
class {{layer_name}}(Tf1xLayer):
    def __init__(self):
        self._scope = '{{layer_name}}'        
        # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
        self._patch_size = {{patch_size}}
        self._feature_maps = {{feature_maps}}
        self._padding = '{{padding}}'
        self._stride = {{stride}}
        
        self._variables = {}
        
    def __call__(self, x):
        shape = [
            self._patch_size,
            self._patch_size,
            x.get_shape().as_list()[-1],
            self._feature_maps
        ]

        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
            initial = tf.random.truncated_normal(
                shape,
                stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
            )
            W = tf.compat.v1.get_variable('W', initializer=initial)
            
            initial = tf.constant(0.1, shape=[self._feature_maps])
            b = tf.compat.v1.get_variable('b', initializer=initial)
            y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding=self._padding) + b
            {% filter remove_lspaces(8) %}
                {% if activation is not none %}
                    y = {{activation}}(y)
                {% endif %}
            {% endfilter %}
            
        self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
        return y

    @property
    def variables(self):
        return self._variables.copy()

    @property
    def trainable_variables(self):
        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
        variables = {v.name: v for v in variables}
        return variables        

    @property
    def weights(self):
        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
            w = tf.compat.v1.get_variable('W')
            return {w.name: w}

    @property
    def biases(self):
        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
            b = tf.compat.v1.get_variable('b')
            return {b.name: b}
{% endmacro %}
