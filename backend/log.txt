2020-04-08 14:28:09,373 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:28:09,374 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:28:09,378 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:28:09,417 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 14:28:49,556 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 14:28:49,558 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1584984632308' with core mode 'v2'
2020-04-08 14:28:49,571 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:49,627 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:49,717 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585039911893' with core mode 'v2'
2020-04-08 14:28:49,717 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:49,767 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:49,874 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585241570197' with core mode 'v2'
2020-04-08 14:28:49,876 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:49,904 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:49,948 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585922031452' with core mode 'v2'
2020-04-08 14:28:49,961 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:50,036 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:50,071 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586189737850' with core mode 'v2'
2020-04-08 14:28:50,076 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:50,100 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:50,133 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 14:28:50,144 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:28:50,167 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:28:50,197 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 14:28:50,204 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:29:05,468 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,513 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:05,553 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,591 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:05,627 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,662 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:05,695 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,727 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:05,770 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,834 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:05,863 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 391, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 14:29:05,910 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 393, in _create_response
    response = self._core.isTrained()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 396, in isTrained
    (self._core_mode == 'v2' and self.core is not None and len(self.core.core_v2.graphs) > 0)
AttributeError: 'coreLogic' object has no attribute 'core'
2020-04-08 14:29:55,207 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:29:55,207 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:29:55,208 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586341956082 [DataData], 1586343034772 [ProcessReshape]
2020-04-08 14:29:55,208 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:29:55,239 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:29:55,240 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:29:55,241 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:29:55,242 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 
2020-04-08 14:29:55,255 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:29:59,446 - INFO - Thread-1 - scraper.py:99 - Persisted 2 scraper entries
2020-04-08 14:30:02,341 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '0' with core mode 'v2'
2020-04-08 14:30:11,208 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:11,209 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:11,210 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:11,210 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:11,220 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:11,220 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 14:30:11,240 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 14:30:11,241 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 14:30:11,325 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
2020-04-08 14:30:11,384 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-04-08 14:30:11,392 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 14:30:11,394 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.15438549999999998 seconds
2020-04-08 14:30:11,397 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:11,398 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:11,398 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:11,399 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:11,399 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:11,400 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:11,401 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:11,401 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:11,417 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:11,418 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:11,420 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:11,420 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:11,427 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:11,440 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:11,440 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:11,441 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:11,441 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:11,442 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 14:30:11,443 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 14:30:11,458 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 14:30:11,532 - INFO - MainThread - networkCache.py:54 - Updating layer 1586348998936
2020-04-08 14:30:11,578 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 14:30:11,579 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.1357882 seconds
2020-04-08 14:30:12,131 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:12,133 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:12,133 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:12,134 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:12,141 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:12,142 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 14:30:12,142 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 14:30:12,143 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 14:30:12,247 - INFO - MainThread - networkCache.py:54 - Updating layer 1586348998936
2020-04-08 14:30:12,308 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 14:30:12,308 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.16687470000000004 seconds
2020-04-08 14:30:12,332 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:12,333 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:12,334 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:12,334 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:12,335 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:12,335 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:12,362 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:12,365 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:12,365 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:12,366 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData]
2020-04-08 14:30:12,377 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:12,378 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:19,463 - INFO - Thread-1 - scraper.py:99 - Persisted 7 scraper entries
2020-04-08 14:30:21,095 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:21,096 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:21,097 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:21,108 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:30:21,109 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:21,110 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:21,724 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:21,725 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:21,725 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:21,726 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:30:21,735 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:21,737 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:21,738 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349018854 [ProcessReshape]
2020-04-08 14:30:21,739 - INFO - MainThread - core.py:202 - ReshapeCodeGenerator
    _permutation : [0, 1, 2]
    _shape       : [28, 28, 1]

2020-04-08 14:30:21,740 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:30:21,746 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854'])
2020-04-08 14:30:21,762 - INFO - MainThread - core.py:193 - Running layer 1586349018854 [ProcessReshape] took 0.02357459999999989 seconds
2020-04-08 14:30:21,786 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:21,787 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:21,788 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:21,788 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:30:21,788 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:21,789 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:21,789 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:30:21,809 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:21,815 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:21,816 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:21,816 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:30:21,817 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:21,817 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:21,818 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:30:29,472 - INFO - Thread-1 - scraper.py:99 - Persisted 4 scraper entries
2020-04-08 14:30:33,522 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:30:33,523 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:30:33,524 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:30:33,524 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349024014 [DeepLearningConv]
2020-04-08 14:30:33,534 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:30:33,546 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:30:33,546 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:30:33,547 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349024014 [DeepLearningConv]
2020-04-08 14:30:33,559 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349024014
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:30:33,579 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:30:33,621 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349024014:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349024014')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349024014')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 14:30:33,636 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349024014'])
2020-04-08 14:30:33,636 - INFO - MainThread - core.py:193 - Running layer 1586349024014 [DeepLearningConv] took 0.07818750000000207 seconds
2020-04-08 14:30:33,637 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:30:34,813 - ERROR - MainThread - script.py:377 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding={{pool_padding}})
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding={{pool_padding}})
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 78, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:30:35,222 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 166, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 78, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:30:39,481 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 14:35:01,908 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:35:01,909 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:35:01,912 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:35:01,945 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 14:35:02,163 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 14:35:02,199 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 14:35:02,200 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:35:07,876 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 14:35:07,969 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:35:07,971 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:35:07,971 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349024014 [DeepLearningConv]
2020-04-08 14:35:07,971 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:35:07,972 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 14:35:07,972 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 14:35:07,972 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 14:35:08,064 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
2020-04-08 14:35:08,146 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-04-08 14:35:08,155 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 14:35:08,155 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.183519 seconds
2020-04-08 14:35:08,166 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349018854 [ProcessReshape]
2020-04-08 14:35:08,168 - INFO - MainThread - core.py:202 - ReshapeCodeGenerator
    _permutation : [0, 1, 2]
    _shape       : [28, 28, 1]

2020-04-08 14:35:08,169 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:35:08,172 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854'])
2020-04-08 14:35:08,173 - INFO - MainThread - core.py:193 - Running layer 1586349018854 [ProcessReshape] took 0.004844500000000002 seconds
2020-04-08 14:35:08,174 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349024014 [DeepLearningConv]
2020-04-08 14:35:08,174 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349024014
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:35:08,188 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:35:08,204 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349024014:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349024014')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349024014')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 14:35:08,252 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349024014'])
2020-04-08 14:35:08,253 - INFO - MainThread - core.py:193 - Running layer 1586349024014 [DeepLearningConv] took 0.07823649999999999 seconds
2020-04-08 14:35:08,266 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:35:11,926 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 14:36:59,859 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:36:59,860 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:36:59,861 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:36:59,863 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349024014 [DeepLearningConv]
2020-04-08 14:36:59,872 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:36:59,873 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:36:59,883 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:36:59,884 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 14:36:59,885 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:36:59,887 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: ['1586349024014']
2020-04-08 14:36:59,932 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:36:59,933 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:36:59,933 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:36:59,934 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:36:59,945 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:36:59,946 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:37:01,994 - INFO - Thread-1 - scraper.py:99 - Persisted 2 scraper entries
2020-04-08 14:37:08,784 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:37:08,785 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:37:08,786 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:37:08,786 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349422756 [DeepLearningConv]
2020-04-08 14:37:08,801 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:37:08,802 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:37:08,803 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:37:08,803 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349422756 [DeepLearningConv]
2020-04-08 14:37:08,805 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349422756
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:37:08,811 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:37:08,838 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349422756:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349422756')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349422756')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 14:37:08,866 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349422756'])
2020-04-08 14:37:08,879 - INFO - MainThread - core.py:193 - Running layer 1586349422756 [DeepLearningConv] took 0.07478829999999448 seconds
2020-04-08 14:37:08,881 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:37:12,003 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 14:37:26,221 - ERROR - MainThread - script.py:377 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 78, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:37:26,718 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 166, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 78, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:38:24,217 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:38:24,218 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:38:24,223 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:38:24,261 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 14:38:24,397 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 14:38:24,415 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 14:38:24,497 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:24,497 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:24,498 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349422756 [DeepLearningConv]
2020-04-08 14:38:24,498 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:24,498 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 14:38:24,499 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 14:38:24,499 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 14:38:24,574 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
2020-04-08 14:38:24,631 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-04-08 14:38:24,640 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 14:38:24,648 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.1490278 seconds
2020-04-08 14:38:24,649 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349018854 [ProcessReshape]
2020-04-08 14:38:24,649 - INFO - MainThread - core.py:202 - ReshapeCodeGenerator
    _permutation : [0, 1, 2]
    _shape       : [28, 28, 1]

2020-04-08 14:38:24,650 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:38:24,652 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854'])
2020-04-08 14:38:24,664 - INFO - MainThread - core.py:193 - Running layer 1586349018854 [ProcessReshape] took 0.01454419999999998 seconds
2020-04-08 14:38:24,664 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349422756 [DeepLearningConv]
2020-04-08 14:38:24,665 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349422756
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:38:24,695 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:38:24,725 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349422756:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349422756')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349422756')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 14:38:24,756 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349422756'])
2020-04-08 14:38:24,768 - INFO - MainThread - core.py:193 - Running layer 1586349422756 [DeepLearningConv] took 0.10348679999999999 seconds
2020-04-08 14:38:24,768 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:38:24,789 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: ['1586349422756']
2020-04-08 14:38:24,858 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:24,858 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:24,859 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 14:38:24,859 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:24,860 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:38:24,861 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:38:24,895 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 14:38:24,897 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:38:32,856 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:38:32,857 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:32,857 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:32,858 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 14:38:32,871 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:32,872 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:38:32,873 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:38:32,873 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349507465 [DeepLearningConv]
2020-04-08 14:38:32,874 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349507465
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : False
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:38:32,894 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:38:32,902 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349507465'])
2020-04-08 14:38:32,906 - INFO - MainThread - core.py:193 - Running layer 1586349507465 [DeepLearningConv] took 0.031549199999998834 seconds
2020-04-08 14:38:32,926 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:38:32,931 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:32,932 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:32,934 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 14:38:32,935 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:32,936 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:38:32,936 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:38:32,937 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 14:38:32,955 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:38:32,961 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:32,962 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:32,963 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 14:38:32,963 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:32,964 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:38:32,965 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:38:32,976 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 14:38:34,245 - INFO - Thread-1 - scraper.py:99 - Persisted 5 scraper entries
2020-04-08 14:38:37,791 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 14:38:37,793 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 14:38:37,793 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 14:38:37,794 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 14:38:37,809 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 14:38:37,810 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 14:38:37,811 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 14:38:37,812 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349507465 [DeepLearningConv]
2020-04-08 14:38:37,813 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349507465
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 14:38:37,833 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 14:38:37,856 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349507465:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349507465')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349507465')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 14:38:37,868 - INFO - MainThread - networkCache.py:54 - Updating layer 1586349507465
2020-04-08 14:38:37,952 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349507465'])
2020-04-08 14:38:37,953 - INFO - MainThread - core.py:193 - Running layer 1586349507465 [DeepLearningConv] took 0.14058849999999978 seconds
2020-04-08 14:38:37,954 - INFO - MainThread - lwInterface.py:255 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 14:38:44,252 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 14:41:53,940 - ERROR - MainThread - script.py:374 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\bdb.py", line 48, in trace_dispatch
    return self.dispatch_line(frame)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\bdb.py", line 67, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2020-04-08 14:41:54,505 - ERROR - MainThread - server.py:191 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 166, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\bdb.py", line 48, in trace_dispatch
    return self.dispatch_line(frame)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\bdb.py", line 67, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2020-04-08 14:41:54,530 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,578 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,613 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,646 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,749 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,819 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,868 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:54,911 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 14:41:55,875 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:42:14,623 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:42:14,624 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:42:14,627 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:42:14,660 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 14:42:15,343 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 14:42:15,395 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 14:42:15,396 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:42:19,421 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 14:42:23,883 - ERROR - MainThread - script.py:377 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:42:24,395 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 166, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 14:44:27,100 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:44:27,100 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:44:27,103 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:44:27,137 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 14:44:27,387 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 14:44:28,225 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 14:44:28,225 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 14:44:29,332 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 14:46:17,814 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 14:46:17,815 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 14:46:17,818 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 14:46:17,855 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 15:00:56,684 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 15:00:56,685 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1584984632308' with core mode 'v2'
2020-04-08 15:00:56,686 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:56,715 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585039911893' with core mode 'v2'
2020-04-08 15:00:56,732 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:56,819 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585241570197' with core mode 'v2'
2020-04-08 15:00:56,823 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:56,885 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585922031452' with core mode 'v2'
2020-04-08 15:00:56,888 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:56,933 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586189737850' with core mode 'v2'
2020-04-08 15:00:56,939 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:56,975 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 15:00:56,984 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:57,065 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 15:00:57,066 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 15:00:58,484 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:58,527 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:58,585 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:58,634 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:58,670 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:58,701 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:00:59,995 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:00:59,996 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:00:59,996 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 15:00:59,997 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:00:59,997 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 15:01:00,022 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 15:01:00,040 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 15:01:00,165 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
2020-04-08 15:01:00,239 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-04-08 15:01:00,248 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 15:01:00,254 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.2327156 seconds
2020-04-08 15:01:00,255 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349018854 [ProcessReshape]
2020-04-08 15:01:00,255 - INFO - MainThread - core.py:202 - ReshapeCodeGenerator
    _permutation : [0, 1, 2]
    _shape       : [28, 28, 1]

2020-04-08 15:01:00,285 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:01:00,302 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854'])
2020-04-08 15:01:00,303 - INFO - MainThread - core.py:193 - Running layer 1586349018854 [ProcessReshape] took 0.046981299999999976 seconds
2020-04-08 15:01:00,304 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349507465 [DeepLearningConv]
2020-04-08 15:01:00,304 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349507465
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 15:01:00,339 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:01:00,358 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349507465:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349507465')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349507465')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 15:01:00,408 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349507465'])
2020-04-08 15:01:00,408 - INFO - MainThread - core.py:193 - Running layer 1586349507465 [DeepLearningConv] took 0.10515429999999998 seconds
2020-04-08 15:01:00,427 - INFO - MainThread - lwInterface.py:256 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 15:01:08,332 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 15:01:09,673 - ERROR - MainThread - script.py:377 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 15:01:10,262 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 167, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 15:04:20,114 - WARNING - MainThread - mainServer.py:72 - No frontend process id specified. Backend will not self terminate if frontend is shutdown unexpectedly.
2020-04-08 15:04:20,115 - INFO - MainThread - mainServer.py:78 - Reporting errors with commit id: Dev
2020-04-08 15:04:20,120 - INFO - MainThread - scraper.py:103 - Starting scraper with handlers: CoreInitHandler, CubeHandler, CpuAndMemHandler, SessionOnRenderHandler
2020-04-08 15:04:20,154 - INFO - MainThread - appServer.py:88 - Trying to listen to: 0.0.0.0 5000
2020-04-08 15:04:24,917 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network 'None' with core mode 'v2'
2020-04-08 15:04:24,922 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1584984632308' with core mode 'v2'
2020-04-08 15:04:24,922 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,014 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585039911893' with core mode 'v2'
2020-04-08 15:04:25,015 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,187 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585241570197' with core mode 'v2'
2020-04-08 15:04:25,188 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,271 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1585922031452' with core mode 'v2'
2020-04-08 15:04:25,272 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,367 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586189737850' with core mode 'v2'
2020-04-08 15:04:25,367 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,433 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '1586260582441' with core mode 'v2'
2020-04-08 15:04:25,433 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:25,458 - INFO - MainThread - coreInterface.py:42 - Created coreLogic for network '' with core mode 'v2'
2020-04-08 15:04:25,466 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 15:04:26,214 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:26,255 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:26,283 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:26,325 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:26,377 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:26,406 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 303, in _create_response
    response = self._core.isRunning()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\coreInterface.py", line 392, in isRunning
    return self.cThread.isAlive()
AttributeError: 'NoneType' object has no attribute 'isAlive'
2020-04-08 15:04:27,304 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:04:27,304 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:04:27,305 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 15:04:27,305 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:04:27,305 - INFO - MainThread - core.py:177 - Preparing layer session for 1586348998936 [DataData]
2020-04-08 15:04:27,321 - INFO - MainThread - codehq.py:37 - Estimated size of data files ['C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy'] is 18.816256 MB. Total memory: 17077.833728 MB
2020-04-08 15:04:27,321 - INFO - MainThread - core.py:202 - DataDataCodeGenerator2
    _layer_id               : 1586348998936
    _seed                   : 0
    batch_size              : 10
    lazy                    : False
    partitions              : [[0.7, 0.2, 0.1]]
    selected_column_indices : []
    shuffle                 : False
    shuffle_buffer_size     : None
    sources                 : [{'type': 'file', 'path': 'C:/Users/Robert/Documents/PerceptiLabs/PereptiLabsPlatform/Data/mnist_split/mnist_input.npy', 'ext': '.npy'}]

2020-04-08 15:04:27,430 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
2020-04-08 15:04:27,504 - WARNING - MainThread - deprecation.py:323 - From C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-04-08 15:04:27,514 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936'])
2020-04-08 15:04:27,520 - INFO - MainThread - core.py:193 - Running layer 1586348998936 [DataData] took 0.2004794 seconds
2020-04-08 15:04:27,521 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349018854 [ProcessReshape]
2020-04-08 15:04:27,522 - INFO - MainThread - core.py:202 - ReshapeCodeGenerator
    _permutation : [0, 1, 2]
    _shape       : [28, 28, 1]

2020-04-08 15:04:27,523 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:04:27,525 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854'])
2020-04-08 15:04:27,536 - INFO - MainThread - core.py:193 - Running layer 1586349018854 [ProcessReshape] took 0.014762799999999993 seconds
2020-04-08 15:04:27,537 - INFO - MainThread - core.py:177 - Preparing layer session for 1586349507465 [DeepLearningConv]
2020-04-08 15:04:27,552 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586349507465
    _padding        : 'SAME'
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 15:04:27,557 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:04:27,592 - INFO - MainThread - errors.py:54 - TypeError("Expected list for 'ksize' argument to 'max_pool' Op, not 2.",) when running layer session 1586349507465:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586349507465')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586349507465')
  7 
  8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding='SAME')
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
->12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 12, in <module>
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 2748, in max_pool
    name=name)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5553, in max_pool
    data_format=data_format, name=name, ctx=_ctx)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 5600, in max_pool_eager_fallback
    "'max_pool' Op, not %r." % ksize)

2020-04-08 15:04:27,640 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586349507465'])
2020-04-08 15:04:27,640 - INFO - MainThread - core.py:193 - Running layer 1586349507465 [DeepLearningConv] took 0.08847519999999998 seconds
2020-04-08 15:04:27,641 - INFO - MainThread - lwInterface.py:256 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 15:04:30,129 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 15:05:55,289 - ERROR - MainThread - script.py:377 - Error when rendering jinja macro tf1x.j2:layer_tf1x_conv. Contents :
  1 {% macro layer_tf1x_switch(layer_name, selected_layer) %}
  2 class {{layer_name}}(Tf1xLayer):
  3     def __init__(self):
  4         self._selected_layer_id = '{{selected_layer}}'
  5     def __call__(self, x):
  6         """ Takes the outputs of all the incoming layers as input and returns the output of that layer."""
  7         y = x[self._selected_layer_id]
  8         return y
  9     @property
 10     def variables(self) -> Dict[str, Picklable]:
 11         """Any variables belonging to this layer that should be rendered in the frontend.
 12         
 13         Returns:
 14             A dictionary with tensor names for keys and picklable for values.
 15         """
 16 
 17         return self._variables.copy()
 18 
 19     @property
 20     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 21         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 22         
 23         Returns:
 24             A dictionary with tensor names for keys and tensors for values.
 25         """
 26         return {}
 27 
 28     @property
 29     def weights(self) -> Dict[str, tf.Tensor]:
 30         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 31 
 32         Return:
 33             A dictionary with tensor names for keys and tensors for values.
 34         """        
 35         return {}
 36 
 37     @property
 38     def biases(self) -> Dict[str, tf.Tensor]:
 39         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 40 
 41         Return:
 42             A dictionary with tensor names for keys and tensors for values.
 43         """        
 44         return {}        
 45 {% endmacro %}
 46 
 47 
 48 
 49 {% macro layer_tf1x_grayscale(layer_name) %}
 50 class {{layer_name}}(Tf1xLayer):
 51     def __call__(self, x: tf.Tensor) -> tf.Tensor:
 52         """ Takes a tensor as input and changes it to grayscale."""
 53         channels = x.get_shape().as_list()[-1]
 54         if channels % 3==0:
 55             if channels>3:
 56                 splits = tf.split(x, int(channels/3), -1)
 57                 images=[]
 58                 for split in splits:
 59                     images.append(tf.image.rgb_to_grayscale(split))
 60                 y = tf.squeeze(tf.stack(images,-1),-2)
 61             else:
 62                 y = tf.image.rgb_to_grayscale(x)
 63         else:
 64             y = x
 65         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
 66         return y
 67 
 68     @property
 69     def variables(self) -> Dict[str, Picklable]:
 70         """Any variables belonging to this layer that should be rendered in the frontend.
 71         
 72         Returns:
 73             A dictionary with tensor names for keys and picklable for values.
 74         """
 75 
 76         return self._variables.copy()
 77 
 78     @property
 79     def trainable_variables(self) -> Dict[str, tf.Tensor]:
 80         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
 81         
 82         Returns:
 83             A dictionary with tensor names for keys and tensors for values.
 84         """
 85         return {}
 86 
 87     @property
 88     def weights(self) -> Dict[str, tf.Tensor]:
 89         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 90 
 91         Return:
 92             A dictionary with tensor names for keys and tensors for values.
 93         """        
 94         return {}
 95 
 96     @property
 97     def biases(self) -> Dict[str, tf.Tensor]:
 98         """Any weight tensors belonging to this layer that should be rendered in the frontend.
 99 
100         Return:
101             A dictionary with tensor names for keys and tensors for values.
102         """        
103         return {}        
104 {% endmacro %}
105 
106 {% macro layer_tf1x_merge(layer_name, type_, merge_dim, merge_order) %}
107 class {{layer_name}}(Tf1xLayer):
108 
109     def __init__(self):
110         self._merge_dim = {{merge_dim}}
111         self._merget_order = {{merge_order}}
112 
113     def __call__(self, x) -> tf.Tensor:
114         """ Takes two tensors as input and merges them accordingly. """
115         {% filter remove_lspaces(8) %}
116             {% if type_ == 'Concat' %}
117                 if self._merge_order is None :
118                     self._merge_order = list(x.values())
119                 for i in range(0, len(self._merge_order), 2):
120                     if not y:
121                         y = list(x.values())[i]
122                    y = tf.concat([y, list(x.values())[i]], self._merge_dim)
123 
124             {% elif type_ == 'Add' %}
125                 for i in range(0, len(list(x.values())), 2):
126                     if not y:
127                         y = list(x.values())[i]
128                     Y = tf.add(list(x.values())[i], y)
129                 
130             {% elif type_ == 'Sub' %}
131                 for i in range(0, len(list(x.values())), 2):
132                     if not y:
133                         y = list(x.values())[i]
134                     y = tf.subtract(list(x.values())[i], y)
135                        
136             {% elif type_ == 'Multi' %}
137                 for i in range(0, len(list(x.values())), 2):
138                     if not y:
139                         y = list(x.values())[i]
140                     y = tf.multiply(list(x.values())[i], y)    
141             {% elif type_ == 'Div' %}
142                 for i in range(0, len(list(x.values())), 2):
143                     if not y:
144                         y = list(x.values())[i]
145                     y = tf.divide(list(x.values())[i], y)
146             {% endif %}
147         {% endfilter %}
148         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
149         return y
150 
151     @property
152     def variables(self) -> Dict[str, Picklable]:
153         """Any variables belonging to this layer that should be rendered in the frontend.
154         
155         Returns:
156             A dictionary with tensor names for keys and picklable for values.
157         """
158 
159         return self._variables.copy()
160 
161     @property
162     def trainable_variables(self) -> Dict[str, tf.Tensor]:
163         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
164         
165         Returns:
166             A dictionary with tensor names for keys and tensors for values.
167         """
168         return {}
169 
170     @property
171     def weights(self) -> Dict[str, tf.Tensor]:
172         """Any weight tensors belonging to this layer that should be rendered in the frontend.
173 
174         Return:
175             A dictionary with tensor names for keys and tensors for values.
176         """        
177         return {}
178 
179     @property
180     def biases(self) -> Dict[str, tf.Tensor]:
181         """Any weight tensors belonging to this layer that should be rendered in the frontend.
182 
183         Return:
184             A dictionary with tensor names for keys and tensors for values.
185         """        
186         return {}        
187 {% endmacro %}
188 
189 {% macro layer_tf1x_word_embedding(layer_name) %}
190 class {{layer_name}}(Tf1xLayer):
191     def __call__(self, x: tf.Tensor) -> tf.Tensor:
192         """ Takes a tensor as input and creates word embedding."""
193         words = tf.string_split(x)
194         vocab_size = words.get_shape().as_list()[0]
195         embed_size=10
196         embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))
197         y = tf.nn.embedding_lookup(embedding, x)
198         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}    
199         return y
200 
201     @property
202     def variables(self) -> Dict[str, Picklable]:
203         """Any variables belonging to this layer that should be rendered in the frontend.
204         
205         Returns:
206             A dictionary with tensor names for keys and picklable for values.
207         """
208 
209         return self._variables.copy()
210 
211     @property
212     def trainable_variables(self) -> Dict[str, tf.Tensor]:
213         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
214         
215         Returns:
216             A dictionary with tensor names for keys and tensors for values.
217         """
218         return {}
219 
220     @property
221     def weights(self) -> Dict[str, tf.Tensor]:
222         """Any weight tensors belonging to this layer that should be rendered in the frontend.
223 
224         Return:
225             A dictionary with tensor names for keys and tensors for values.
226         """        
227         return {}
228 
229     @property
230     def biases(self) -> Dict[str, tf.Tensor]:
231         """Any weight tensors belonging to this layer that should be rendered in the frontend.
232 
233         Return:
234             A dictionary with tensor names for keys and tensors for values.
235         """        
236         return {}        
237 {% endmacro %}
238 
239 
240 
241 {% macro layer_tf1x_reshape(layer_name, shape, permutation) %}
242 class {{layer_name}}(Tf1xLayer):
243     def __call__(self, x: tf.Tensor) -> tf.Tensor:
244         """ Takes a tensor as input and reshapes it."""
245         y = tf.reshape(x, [-1] + {{shape}})
246         y = tf.transpose(y, perm=[0] + [i+1 for i in {{permutation}}])
247         return y
248 
249     @property
250     def variables(self) -> Dict[str, Picklable]:
251         """Any variables belonging to this layer that should be rendered in the frontend.
252         
253         Returns:
254             A dictionary with tensor names for keys and picklable for values.
255         """
256         return {}
257 
258     @property
259     def trainable_variables(self) -> Dict[str, tf.Tensor]:
260         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
261         
262         Returns:
263             A dictionary with tensor names for keys and tensors for values.
264         """
265         return {}
266 
267     @property
268     def weights(self) -> Dict[str, tf.Tensor]:
269         """Any weight tensors belonging to this layer that should be rendered in the frontend.
270 
271         Return:
272             A dictionary with tensor names for keys and tensors for values.
273         """        
274         return {}
275 
276     @property
277     def biases(self) -> Dict[str, tf.Tensor]:
278         """Any weight tensors belonging to this layer that should be rendered in the frontend.
279 
280         Return:
281             A dictionary with tensor names for keys and tensors for values.
282         """        
283         return {}        
284 {% endmacro %}
285 
286 
287 {% macro layer_tf1x_one_hot(layer_name, n_classes) %}
288 class {{layer_name}}(Tf1xLayer):
289     def __call__(self, x):
290         y = tf.one_hot(tf.cast(x, dtype=tf.int32), {{n_classes}})        
291         return y
292 
293     @property
294     def variables(self):
295         """Any variables belonging to this layer that should be rendered in the frontend.
296         
297         Returns:
298             A dictionary with tensor names for keys and picklable for values.
299         """
300         return {}
301 
302     @property
303     def trainable_variables(self):
304         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
305         
306         Returns:
307             A dictionary with tensor names for keys and tensors for values.
308         """
309         return {}
310 
311     @property
312     def weights(self):
313         """Any weight tensors belonging to this layer that should be rendered in the frontend.
314 
315         Return:
316             A dictionary with tensor names for keys and tensors for values.
317         """        
318         return {}
319 
320     @property    
321     def biases(self):
322         """Any weight tensors belonging to this layer that should be rendered in the frontend.
323 
324         Return:
325             A dictionary with tensor names for keys and tensors for values.
326         """        
327         return {}    
328 {% endmacro %}
329 
330 
331 {% macro layer_tf1x_fully_connected(layer_name, n_neurons, activation, dropout, keep_prob) %}
332 class {{layer_name}}(Tf1xLayer):
333     def __init__(self):
334         self._scope = '{{layer_name}}'
335         self._n_neurons = 10
336         self._variables = {}
337         
338     def __call__(self, x: tf.Tensor):
339         """ Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor."""        
340         self._n_neurons = {{n_neurons}}
341         n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)
342 
343         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):        
344             initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)
345             W = tf.compat.v1.get_variable('W', initializer=initial)
346         
347             initial = tf.constant(0.1, shape=[self._n_neurons])
348             b = tf.compat.v1.get_variable('b', initializer=initial)
349             flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)
350             y = tf.matmul(flat_node, W) + b
351 
352             {% filter remove_lspaces(8) %}
353                 {% if activation is not none %}
354                     y = {{activation}}(y)
355                 {% endif %}
356             {% endfilter %}
357             
358         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
359         return y
360 
361     @property
362     def variables(self):
363         """Any variables belonging to this layer that should be rendered in the frontend.
364         
365         Returns:
366             A dictionary with tensor names for keys and picklable for values.
367         """
368         return self._variables.copy()
369 
370     @property
371     def trainable_variables(self):
372         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
373         
374         Returns:
375             A dictionary with tensor names for keys and tensors for values.
376         """
377         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
378         variables = {v.name: v for v in variables}
379         return variables
380 
381     @property
382     def weights(self):
383         """Any weight tensors belonging to this layer that should be rendered in the frontend.
384 
385         Return:
386             A dictionary with tensor names for keys and tensors for values.
387         """        
388         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
389             w = tf.compat.v1.get_variable('W')
390             return {w.name: w}
391 
392     @property
393     def biases(self):
394         """Any weight tensors belonging to this layer that should be rendered in the frontend.
395 
396         Return:
397             A dictionary with tensor names for keys and tensors for values.
398         """        
399         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
400             b = tf.compat.v1.get_variable('b')
401             return {b.name: b}
402     
403 {% endmacro %}
404 
405 
406 {% macro layer_tf1x_conv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation, pool, pooling, pool_padding, pool_area, pool_stride) %}
407 class {{layer_name}}(Tf1xLayer):
408     def __init__(self):
409         self._scope = '{{layer_name}}'        
410         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc
411         self._patch_size = {{patch_size}}
412         self._feature_maps = {{feature_maps}}
413         self._padding = '{{padding}}'
414         self._stride = {{stride}}
415         self._keep_prob = {{keep_prob}}
416         self._variables = {}
417         
418     def __call__(self, x):
419         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                
420         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
421             {% filter remove_lspaces(8) %}
422                 {% if conv_dim == '2D' %}
423                     shape = [
424                     self._patch_size,
425                     self._patch_size,
426                     x.get_shape().as_list()[-1],
427                     self._feature_maps
428                     ]
429                     initial = tf.random.truncated_normal(
430                         shape,
431                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
432                     )
433                     W = tf.compat.v1.get_variable('W', initializer=initial)
434                     
435                     initial = tf.constant(0.1, shape=[self._feature_maps])
436                     b = tf.compat.v1.get_variable('b', initializer=initial)
437                     y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')
438                 {% elif conv_dim == '1D' %}
439                     shape = [
440                     self._patch_size,
441                     x.get_shape().as_list()[-1],
442                     self._feature_maps
443                     ]
444                     initial = tf.random.truncated_normal(
445                         shape,
446                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
447                     )
448                     W = tf.compat.v1.get_variable('W', initializer=initial)
449                     
450                     initial = tf.constant(0.1, shape=[self._feature_maps])
451                     b = tf.compat.v1.get_variable('b', initializer=initial)
452                     y = tf.nn.conv1d(x, W, strides=[1, self._stride, 1], padding=self._padding)
453                 {% elif conv_dim == '3D' %}
454                     shape = [
455                     self._patch_size,
456                     self._patch_size,
457                     self._patch_size,
458                     x.get_shape().as_list()[-1],
459                     self._feature_maps
460                     ]
461                     initial = tf.random.truncated_normal(
462                         shape,
463                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
464                     )
465                     W = tf.compat.v1.get_variable('W', initializer=initial)
466                     
467                     initial = tf.constant(0.1, shape=[self._feature_maps])
468                     b = tf.compat.v1.get_variable('b', initializer=initial)
469                     y = tf.nn.conv3d(x, W, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
470                 {% elif conv_dim == 'Automatic' %}
471                     dim = len(x.get_shape().as_list())-1
472                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
473                     initial = tf.random.truncated_normal(
474                         shape,
475                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
476                     )
477                     W = tf.compat.v1.get_variable('W', initializer=initial)
478                     
479                     initial = tf.constant(0.1, shape=[self._feature_maps])
480                     b = tf.compat.v1.get_variable('b', initializer=initial)
481                     y = tf.nn.conv2d(x, W, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
482                 {% endif %}
483             {% endfilter %}
484             {% filter remove_lspaces(8) %}
485                 {% if dropout %}
486                     y = tf.nn.dropout(y,self._keep_prob)
487                 {% endif %}
488             {% endfilter %}
489             {% filter remove_lspaces(8) %}
490                 {% if activation is not none %}
491                     y = y + b
492                     y = {{activation}}(y)
493                 {% endif %}
494             {% endfilter %}
495             {% filter remove_lspaces(8) %}
496                 {% if pool and pooling == "Max" %}
497                     y = tf.nn.max_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
498                 {% endif %}
499             {% endfilter %}
500             {% filter remove_lspaces(8) %}
501                 {% if pool and pooling == "Mean" %}
502                     y = tf.nn.avg_pool(y, ksize={{pool_area}}, stride={{pool_stride}}, padding='SAME')
503                 {% endif %}
504             {% endfilter %}
505         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
506         return y
507 
508     @property
509     def variables(self):
510         """Any variables belonging to this layer that should be rendered in the frontend.
511         
512         Returns:
513             A dictionary with tensor names for keys and picklable for values.
514         """
515         return self._variables.copy()
516 
517     @property
518     def trainable_variables(self):
519         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
520         
521         Returns:
522             A dictionary with tensor names for keys and tensors for values.
523         """
524         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
525         variables = {v.name: v for v in variables}
526         return variables        
527 
528     @property
529     def weights(self):
530         """Any weight tensors belonging to this layer that should be rendered in the frontend.
531 
532         Return:
533             A dictionary with tensor names for keys and tensors for values.
534         """        
535         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
536             w = tf.compat.v1.get_variable('W')
537             return {w.name: w}
538 
539     @property
540     def biases(self):
541         """Any weight tensors belonging to this layer that should be rendered in the frontend.
542 
543         Return:
544             A dictionary with tensor names for keys and tensors for values.
545         """        
546         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
547             b = tf.compat.v1.get_variable('b')
548             return {b.name: b}
549 {% endmacro %}
550 
551 {% macro layer_tf1x_deconv(layer_name, conv_dim, patch_size, feature_maps, stride, padding, dropout, keep_prob, activation) %}
552 class {{layer_name}}(Tf1xLayer):
553     def __init__(self):
554         self._scope = '{{layer_name}}'        
555         self._patch_size = {{patch_size}}
556         self._feature_maps = {{feature_maps}}
557         self._padding = '{{padding}}'
558         self._stride = {{stride}}
559         self._keep_prob = {{keep_prob}}
560         self._variables = {}
561         
562     def __call__(self, x):
563         """ Takes a tensor as input and feeds it forward through a deconvolutional layer, returning a newtensor."""                
564         
565         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
566             {% filter remove_lspaces(8) %}
567                 {% if conv_dim == '2D' %}
568                     shape = [
569                     self._patch_size,
570                     self._patch_size,
571                     x.get_shape().as_list()[-1],
572                     self._feature_maps
573                     ]
574                     initial = tf.random.truncated_normal(
575                         shape,
576                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
577                     )
578                     W = tf.compat.v1.get_variable('W', initializer=initial)
579                     
580                     initial = tf.constant(0.1, shape=[self._feature_maps])
581                     b = tf.compat.v1.get_variable('b', initializer=initial)
582                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
583                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, 1], padding=self._padding)
584 
585                 {% elif conv_dim == '1D' %}
586                     shape = [
587                     self._patch_size,
588                     x.get_shape().as_list()[-1],
589                     self._feature_maps
590                     ]
591                     initial = tf.random.truncated_normal(
592                         shape,
593                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
594                     )
595                     W = tf.compat.v1.get_variable('W', initializer=initial)
596                     
597                     initial = tf.constant(0.1, shape=[self._feature_maps])
598                     b = tf.compat.v1.get_variable('b', initializer=initial)
599                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
600 
601                     y = tf.nn.conv1d_transpose(x, W, output_shape, strides=[1, self._stride, 1], padding=self._padding)
602 
603                 {% elif conv_dim == '3D' %}
604                     shape = [
605                     self._patch_size,
606                     self._patch_size,
607                     self._patch_size,
608                     x.get_shape().as_list()[-1],
609                     self._feature_maps
610                     ]
611                     initial = tf.random.truncated_normal(
612                         shape,
613                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
614                     )
615                     W = tf.compat.v1.get_variable('W', initializer=initial)
616                     
617                     initial = tf.constant(0.1, shape=[self._feature_maps])
618                     b = tf.compat.v1.get_variable('b', initializer=initial)
619                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
620                     y = tf.nn.conv3d_transpose(x, W, output_shape, strides=[1, self._stride, self._stride, self._stride, 1], padding=self._padding)
621 
622                 {% elif conv_dim == 'Automatic' %}
623                     dim = len(x.get_shape().as_list())-1
624                     shape = [self._patch_size]*dim + [x.get_shape().as_list()[-1], self._feature_maps]
625                     initial = tf.random.truncated_normal(
626                         shape,
627                         stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)
628                     )
629                     W = tf.compat.v1.get_variable('W', initializer=initial)
630                     
631                     initial = tf.constant(0.1, shape=[self._feature_maps])
632                     b = tf.compat.v1.get_variable('b', initializer=initial)
633                     output_shape = tf.stack([x.get_shape().as_list()[0]] + [node_shape*self._stride for node_shape in  x.get_shape().as_list()[1:-1]] + [self._feature_maps])   
634                     y = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1]+ [self._stride]*dim +[1], padding=self._padding)
635 
636                 {% endif %}
637             {% endfilter %}
638             {% filter remove_lspaces(8) %}
639                 {% if dropout %}
640                     y = tf.nn.dropout(y,self._keep_prob)
641                 {% endif %}
642                 {% if activation is not none %}
643                     y = y + b
644                     y = {{activation}}(y)
645                 {% endif %}
646             {% endfilter %}
647             
648         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}
649         return y
650 
651     @property
652     def variables(self):
653         """Any variables belonging to this layer that should be rendered in the frontend.
654         
655         Returns:
656             A dictionary with tensor names for keys and picklable for values.
657         """
658         return self._variables.copy()
659 
660     @property
661     def trainable_variables(self):
662         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
663         
664         Returns:
665             A dictionary with tensor names for keys and tensors for values.
666         """
667         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
668         variables = {v.name: v for v in variables}
669         return variables        
670 
671     @property
672     def weights(self):
673         """Any weight tensors belonging to this layer that should be rendered in the frontend.
674 
675         Return:
676             A dictionary with tensor names for keys and tensors for values.
677         """        
678         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
679             w = tf.compat.v1.get_variable('W')
680             return {w.name: w}
681 
682     @property
683     def biases(self):
684         """Any weight tensors belonging to this layer that should be rendered in the frontend.
685 
686         Return:
687             A dictionary with tensor names for keys and tensors for values.
688         """        
689         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
690             b = tf.compat.v1.get_variable('b')
691             return {b.name: b}
692 {% endmacro %}
693 
694 {% macro layer_tf1x_recurrent(layer_name, version, time_steps, neurons, return_sequences, dropout, keep_prop) %}
695 class {{layer_name}}(Tf1xLayer):
696     def __init__(self):
697         self._scope = '{{layer_name}}'
698         self._variables = {}
699         self._neurons = {{neurons}}
700     def __call__(self, x: tf.Tensor):
701         """ Takes a tensor as input and feeds it forward through a recurrent layer, returning a newtensor."""        
702 
703         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):     
704             {% filter remove_lspaces(8) %}   
705                 {% if version == 'LSTM' %}
706                     cell = tf.nn.rnn_cell.LSTMCell(self._neurons, state_is_tuple=True, name=self._scope)
707                 {% elif version == 'GRU' %}
708                     cell = tf.nn.rnn_cell.GRUCell(self._neurons, state_is_tuple=True, name=self._scope)
709                 {% elif version == 'RNN' %}
710                     cell = tf.nn.rnn_cell.RNNCell(self._neurons, state_is_tuple=True, name=self._scope)
711                 {% endif %}
712             {% endfilter %}
713             {% filter remove_lspaces(8) %}
714                 {% if dropout %}
715                     cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self._keep_prob)
716                 {% endif %}
717             {% endfilter %}
718             node = x
719             rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, node, dtype=node.dtype)
720             {% filter remove_lspaces(8) %}
721                 {% if return_sequences %}
722                     y = rnn_outputs
723                 {% else %}
724                     y = rnn_outputs[:, -1]
725                 {% endif %}
726             {% endfilter %}
727             
728         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}            
729         return y
730 
731     @property
732     def variables(self):
733         """Any variables belonging to this layer that should be rendered in the frontend.
734         
735         Returns:
736             A dictionary with tensor names for keys and picklable for values.
737         """
738         return self._variables.copy()
739 
740     @property
741     def trainable_variables(self):
742         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.
743         
744         Returns:
745             A dictionary with tensor names for keys and tensors for values.
746         """
747         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)
748         variables = {v.name: v for v in variables}
749         return variables
750 
751     @property
752     def weights(self):
753         """Any weight tensors belonging to this layer that should be rendered in the frontend.
754 
755         Return:
756             A dictionary with tensor names for keys and tensors for values.
757         """        
758         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
759             w = tf.compat.v1.get_variable('W')
760             return {w.name: w}
761 
762     @property
763     def biases(self):
764         """Any weight tensors belonging to this layer that should be rendered in the frontend.
765 
766         Return:
767             A dictionary with tensor names for keys and tensors for values.
768         """        
769         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):
770             b = tf.compat.v1.get_variable('b')
771             return {b.name: b}
772     
773 {% endmacro %}
774 
Traceback (most recent call last):
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 15:05:55,840 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 105, in interface
    response, warnings, errors = self._interface.create_response(self.request)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 159, in create_response
    response = self._create_response(reciever, action, value)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\mainInterface.py", line 217, in _create_response
    return get_code.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\lwInterface.py", line 167, in run
    code = script_factory.render_layer_code(node.layer_id, node.layer_type, node.layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 347, in render_layer_code
    code = self._render_layer_macro(layer_id, layer_type, layer_spec)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\script.py", line 374, in _render_layer_macro
    code = self._engine.render_string(template)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\layers\templates\base.py", line 79, in render_string
    text = self._jenv.from_string(code).render(**kwargs)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 880, in from_string
    return cls.from_code(self, self.compile(source), globals, None)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 591, in compile
    self.handle_exception(exc_info, source_hint=source_hint)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "<unknown>", line 2, in template
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\environment.py", line 497, in _parse
    return Parser(self, source, name, encode_filename(filename)).parse()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 901, in parse
    result = nodes.Template(self.subparse(), lineno=1)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 875, in subparse
    add_data(self.parse_tuple(with_condexpr=True))
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 620, in parse_tuple
    args.append(parse())
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 432, in parse_expression
    return self.parse_condexpr()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 437, in parse_condexpr
    expr1 = self.parse_or()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 450, in parse_or
    left = self.parse_and()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 459, in parse_and
    left = self.parse_not()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 470, in parse_not
    return self.parse_compare()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 474, in parse_compare
    expr = self.parse_math1()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 496, in parse_math1
    left = self.parse_concat()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 507, in parse_concat
    args = [self.parse_math2()]
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 517, in parse_math2
    left = self.parse_pow()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 528, in parse_pow
    left = self.parse_unary()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 547, in parse_unary
    node = self.parse_postfix(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 676, in parse_postfix
    node = self.parse_call(node)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\parser.py", line 767, in parse_call
    self.stream.expect('comma')
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\jinja2\lexer.py", line 384, in expect
    self.name, self.filename)
jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'SAME'
2020-04-08 15:05:55,892 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 15:05:55,939 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 15:05:55,986 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 15:05:56,020 - ERROR - MainThread - server.py:193 - Error in connection handler
Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 827, in transfer_data
    message = await self.read_message()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 895, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 971, in read_data_frame
    frame = await self.read_frame(max_size)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 1051, in read_frame
    extensions=self.extensions,
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\framing.py", line 105, in read
    data = await reader(2)
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\asyncio\streams.py", line 666, in readexactly
    raise IncompleteReadError(incomplete, n)
asyncio.streams.IncompleteReadError: 0 bytes read on a total of 2 expected bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\server.py", line 191, in handler
    await self.ws_handler(self, path)
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\server\web_serverlib.py", line 100, in interface
    self.request = await websocket.recv()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 509, in recv
    await self.ensure_open()
  File "C:\Users\Robert\Anaconda3\envs\baseCore\lib\site-packages\websockets\protocol.py", line 803, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: code = 1006 (connection closed abnormally [internal]), no reason
2020-04-08 15:05:56,409 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:05:56,410 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:05:56,410 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:05:56,411 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586349507465 [DeepLearningConv]
2020-04-08 15:05:56,415 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:05:56,415 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:05:56,416 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:05:56,416 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:05:56,417 - INFO - MainThread - lwInterface.py:256 - ErrorMessage: ErrorDescription(error_message="TypeError at line 12: Expected list for 'ksize' argument to 'max_pool' Op, not 2.", error_line='12')
2020-04-08 15:05:56,422 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: ['1586349507465']
2020-04-08 15:05:56,470 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:05:56,471 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:05:56,471 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 15:05:56,472 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:05:56,480 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:05:56,481 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:05:56,552 - INFO - MainThread - mainInterface.py:412 - User has been set to robert.l@perceptilabs.com
2020-04-08 15:06:00,181 - INFO - Thread-1 - scraper.py:99 - Persisted 2 scraper entries
2020-04-08 15:06:00,376 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:00,377 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:00,378 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:00,378 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:00,385 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:00,386 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:00,388 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:00,388 - INFO - MainThread - core.py:177 - Preparing layer session for 1586351145241 [DeepLearningConv]
2020-04-08 15:06:00,389 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586351145241
    _padding        : SAME
    _patch_size     : 3
    _pool           : False
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 15:06:00,392 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:06:00,404 - INFO - MainThread - errors.py:54 - NameError("name 'SAME' is not defined",) when running layer session 1586351145241:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586351145241')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586351145241')
  7 
->8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding=SAME)
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 8, in <module>

2020-04-08 15:06:00,413 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586351145241'])
2020-04-08 15:06:00,417 - INFO - MainThread - core.py:193 - Running layer 1586351145241 [DeepLearningConv] took 0.028364999999993756 seconds
2020-04-08 15:06:00,418 - INFO - MainThread - lwInterface.py:256 - ErrorMessage: ErrorDescription(error_message="NameError at line 8: name 'SAME' is not defined", error_line='8')
2020-04-08 15:06:10,189 - INFO - Thread-1 - scraper.py:99 - Persisted 1 scraper entries
2020-04-08 15:06:14,936 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:14,937 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:14,938 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:14,946 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:14,946 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:14,947 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:14,955 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:14,956 - INFO - MainThread - core.py:177 - Preparing layer session for 1586351145241 [DeepLearningConv]
2020-04-08 15:06:14,957 - INFO - MainThread - core.py:202 - CustomCodeGenerator
Code:
1 class DeepLearningConv_Convolution_1(Tf1xLayer):2     def __init__(self):3         self._scope = 'DeepLearningConv_Convolution_1'        4         # TODO: implement support for 1d and 3d conv, dropout, funcs, pooling, etc5         self._patch_size = 36         self._feature_maps = 87         self._padding = 'AM'8         self._stride = 29         self._keep_prob = 110         self._variables = {}11         12     def __call__(self, x):13         """ Takes a tensor as input and feeds it forward through a convolutional layer, returning a newtensor."""                14         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):15             shape = [16             self._patch_size,17             self._patch_size,18             x.get_shape().as_list()[-1],19             self._feature_maps20             ]21             initial = tf.random.truncated_normal(22                 shape,23                 stddev=np.sqrt(2/(self._patch_size)**2 + self._feature_maps)24             )25             W = tf.compat.v1.get_variable('W', initializer=initial)26             27             initial = tf.constant(0.1, shape=[self._feature_maps])28             b = tf.compat.v1.get_variable('b', initializer=initial)29             y = tf.nn.conv2d(x, W, strides=[1, self._stride, self._stride, 1], padding='SAME')30             y = y + b31             y = tf.compat.v1.sigmoid(y)32         self._variables = {k: v for k, v in locals().items() if dill.pickles(v)}33         return y34 35     @property36     def variables(self):37         """Any variables belonging to this layer that should be rendered in the frontend.38         39         Returns:40             A dictionary with tensor names for keys and picklable for values.41         """42         return self._variables.copy()43 44     @property45     def trainable_variables(self):46         """Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.47         48         Returns:49             A dictionary with tensor names for keys and tensors for values.50         """51         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)52         variables = {v.name: v for v in variables}53         return variables        54 55     @property56     def weights(self):57         """Any weight tensors belonging to this layer that should be rendered in the frontend.58 59         Return:60             A dictionary with tensor names for keys and tensors for values.61         """        62         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):63             w = tf.compat.v1.get_variable('W')64             return {w.name: w}65 66     @property67     def biases(self):68         """Any weight tensors belonging to this layer that should be rendered in the frontend.69 70         Return:71             A dictionary with tensor names for keys and tensors for values.72         """        73         with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):74             b = tf.compat.v1.get_variable('b')75             return {b.name: b}76 77 
2020-04-08 15:06:14,962 - WARNING - MainThread - core.py:238 - Custom code not supported in lightweight core for core mode == 'v2'. Replacing generated code with identity (Y = X['Y'])
2020-04-08 15:06:14,965 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:06:14,974 - INFO - MainThread - networkCache.py:54 - Updating layer 1586351145241
2020-04-08 15:06:15,041 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586351145241'])
2020-04-08 15:06:15,042 - INFO - MainThread - core.py:193 - Running layer 1586351145241 [DeepLearningConv] took 0.08499840000000347 seconds
2020-04-08 15:06:15,070 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:15,071 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:15,071 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:15,071 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:15,072 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:15,072 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:15,072 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:15,073 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:15,096 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:15,099 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:15,100 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:15,100 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:15,101 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:15,101 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:15,102 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:15,103 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:17,748 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:17,749 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:17,750 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:17,750 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:17,758 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:17,759 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:17,760 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:17,761 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:17,785 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:17,786 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:17,787 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:17,787 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:17,788 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:17,788 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:17,797 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:17,800 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:17,819 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:17,825 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:17,826 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:17,827 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:17,828 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:17,828 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:17,829 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:17,829 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:20,199 - INFO - Thread-1 - scraper.py:99 - Persisted 6 scraper entries
2020-04-08 15:06:21,639 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:21,640 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:21,640 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:21,641 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351145241 [DeepLearningConv]
2020-04-08 15:06:21,647 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:21,647 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:21,648 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:21,649 - INFO - MainThread - core.py:173 - Using cached layer for layer DeepLearningConv
2020-04-08 15:06:21,653 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: ['1586351145241']
2020-04-08 15:06:21,697 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:21,698 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:21,698 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape]
2020-04-08 15:06:21,699 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:21,706 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:21,711 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:27,722 - INFO - MainThread - mainInterface.py:79 - Deleting these layers: []
2020-04-08 15:06:27,723 - INFO - MainThread - cache.py:59 - Cache invalidation removed 0/0 entries
2020-04-08 15:06:27,723 - INFO - MainThread - core.py:321 - Running core [LightweightCore]
2020-04-08 15:06:27,724 - INFO - MainThread - core.py:324 - Layers will be executed in the following order: 1586348998936 [DataData], 1586349018854 [ProcessReshape], 1586351183317 [DeepLearningConv]
2020-04-08 15:06:27,731 - INFO - MainThread - core.py:328 - Module hooks installed are: tf.placeholder
2020-04-08 15:06:27,732 - INFO - MainThread - core.py:173 - Using cached layer for layer DataData
2020-04-08 15:06:27,733 - INFO - MainThread - core.py:173 - Using cached layer for layer ProcessReshape
2020-04-08 15:06:27,740 - INFO - MainThread - core.py:177 - Preparing layer session for 1586351183317 [DeepLearningConv]
2020-04-08 15:06:27,747 - INFO - MainThread - core.py:202 - ConvCodeGenerator
    _activation     : Sigmoid
    _conv_dim       : 2D
    _dropout        : False
    _feature_maps   : 8
    _keep_prob      : 1
    _layer_id       : 1586351183317
    _padding        : SAME
    _patch_size     : 3
    _pool           : True
    _pool_area      : 2
    _pool_padding   : None
    _pool_stride    : 2
    _pooling        : Max
    _stride         : 2
    _variable_scope : None

2020-04-08 15:06:27,753 - WARNING - MainThread - session.py:101 - Overwriting existing, non-identical, api in globals
2020-04-08 15:06:27,761 - INFO - MainThread - errors.py:54 - NameError("name 'SAME' is not defined",) when running layer session 1586351183317:
  1 shape = [3, 3, X['Y'].get_shape().as_list()[-1], 8]
  2 initial = tf.truncated_normal(shape, stddev=np.sqrt(2/(3**2 + 8)))
  3 W = tf.Variable(initial, name='weights-1586351183317')
  4 
  5 initial = tf.constant(0.1, shape=[8])
  6 b = tf.Variable(initial, name='bias-1586351183317')
  7 
->8 node = tf.nn.conv2d(X['Y'], W, strides=[1, 2, 2, 1], padding=SAME)
  9 node = node + b
  10 Y = tf.sigmoid(node)
  11 dim_str = '2D'
  12 Y = tf.nn.max_pool(Y, 2, 2, 'None', dim_str)

  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\core.py", line 255, in _run_layer
    session.run()
  File "C:\Users\Robert\Documents\PerceptiLabs\PereptiLabsPlatform\PerceptiLabs\backend\perceptilabs\core_new\session.py", line 69, in run
    exec(self._code, global_vars, local_vars)
  File "<string>", line 8, in <module>

2020-04-08 15:06:27,782 - INFO - MainThread - networkCache.py:59 - Cached layers: dict_keys(['1586348998936', '1586349018854', '1586351183317'])
2020-04-08 15:06:27,787 - INFO - MainThread - core.py:193 - Running layer 1586351183317 [DeepLearningConv] took 0.039947200000000294 seconds
2020-04-08 15:06:27,788 - INFO - MainThread - lwInterface.py:256 - ErrorMessage: ErrorDescription(error_message="NameError at line 8: name 'SAME' is not defined", error_line='8')
2020-04-08 15:06:30,206 - INFO - Thread-1 - scraper.py:99 - Persisted 3 scraper entries
